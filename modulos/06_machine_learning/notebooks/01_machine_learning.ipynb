{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7fcff48",
   "metadata": {},
   "source": [
    "# ü§ñ Machine Learning con Python - M√≥dulo 6\n",
    "\n",
    "## Bienvenido al M√≥dulo de Machine Learning\n",
    "\n",
    "### üìö Contenido del M√≥dulo 6:\n",
    "1. **Introducci√≥n al Machine Learning**\n",
    "2. **Preparaci√≥n de datos para ML**\n",
    "3. **Algoritmos de aprendizaje supervisado**\n",
    "4. **Algoritmos de aprendizaje no supervisado**\n",
    "5. **Evaluaci√≥n y validaci√≥n de modelos**\n",
    "6. **Optimizaci√≥n de hiperpar√°metros**\n",
    "7. **Proyecto: Sistema de recomendaciones**\n",
    "\n",
    "### üéØ Objetivos de Aprendizaje:\n",
    "- Entender los conceptos fundamentales del ML\n",
    "- Dominar scikit-learn para construir modelos\n",
    "- Implementar algoritmos de clasificaci√≥n y regresi√≥n\n",
    "- Realizar clustering y an√°lisis de componentes principales\n",
    "- Evaluar y optimizar modelos de ML\n",
    "- Aplicar t√©cnicas de feature engineering\n",
    "- Construir un sistema de recomendaciones completo\n",
    "\n",
    "### üõ†Ô∏è Tecnolog√≠as y bibliotecas:\n",
    "- **Scikit-learn**: Framework principal de ML\n",
    "- **XGBoost**: Gradient boosting avanzado\n",
    "- **Pandas & NumPy**: Manipulaci√≥n de datos\n",
    "- **Matplotlib & Seaborn**: Visualizaci√≥n\n",
    "- **Plotly**: Visualizaciones interactivas\n",
    "- **Joblib**: Persistencia de modelos\n",
    "- **SHAP**: Explicabilidad de modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b0ab64",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. üß† Introducci√≥n al Machine Learning\n",
    "\n",
    "El Machine Learning es una rama de la inteligencia artificial que permite a las computadoras aprender y hacer predicciones o decisiones sin ser expl√≠citamente programadas para cada tarea espec√≠fica.\n",
    "\n",
    "### üåü Tipos de Machine Learning:\n",
    "\n",
    "1. **Aprendizaje Supervisado**: Aprendemos de datos etiquetados\n",
    "   - **Clasificaci√≥n**: Predecir categor√≠as (spam/no spam, diagn√≥stico m√©dico)\n",
    "   - **Regresi√≥n**: Predecir valores num√©ricos (precio de casa, temperatura)\n",
    "\n",
    "2. **Aprendizaje No Supervisado**: Encontrar patrones sin etiquetas\n",
    "   - **Clustering**: Agrupar datos similares (segmentaci√≥n de clientes)\n",
    "   - **Reducci√≥n de dimensionalidad**: Simplificar datos (PCA, t-SNE)\n",
    "\n",
    "3. **Aprendizaje por Refuerzo**: Aprender a trav√©s de recompensas\n",
    "   - Agentes que interact√∫an con un entorno (juegos, rob√≥tica)\n",
    "\n",
    "### üîÑ Flujo de trabajo t√≠pico en ML:\n",
    "\n",
    "1. **Definici√≥n del problema**: ¬øQu√© queremos predecir?\n",
    "2. **Recolecci√≥n de datos**: Obtener datos relevantes y suficientes\n",
    "3. **Exploraci√≥n y limpieza**: Entender y preparar los datos\n",
    "4. **Feature engineering**: Crear variables relevantes\n",
    "5. **Selecci√≥n del modelo**: Elegir algoritmos apropiados\n",
    "6. **Entrenamiento**: Ajustar el modelo a los datos\n",
    "7. **Evaluaci√≥n**: Medir el rendimiento del modelo\n",
    "8. **Optimizaci√≥n**: Mejorar el modelo\n",
    "9. **Despliegue**: Poner el modelo en producci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b21af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar las bibliotecas principales para Machine Learning\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Scikit-learn: Framework principal de ML\n",
    "from sklearn.datasets import make_classification, make_regression, load_iris, load_boston\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import mean_squared_error, r2_score, classification_report, confusion_matrix\n",
    "\n",
    "# Algoritmos de ML\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression, Ridge, Lasso\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Manejo de modelos\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuraci√≥n para visualizaciones\n",
    "plt.style.use('ggplot')\n",
    "sns.set(style=\"whitegrid\")\n",
    "np.random.seed(42)\n",
    "\n",
    "# Mostrar versiones\n",
    "print(\"üõ†Ô∏è Bibliotecas de Machine Learning cargadas:\")\n",
    "print(f\"üìä Pandas: {pd.__version__}\")\n",
    "print(f\"üî¢ NumPy: {np.__version__}\")\n",
    "print(f\"ü§ñ Scikit-learn: {sklearn.__version__}\")\n",
    "print(f\"üìà Matplotlib: {plt.__version__}\")\n",
    "print(f\"üé® Seaborn: {sns.__version__}\")\n",
    "\n",
    "print(\"\\n‚úÖ ¬°Entorno de Machine Learning listo!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45864329",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. üìä Preparaci√≥n de datos para ML\n",
    "\n",
    "La calidad de los datos determina el √©xito de cualquier proyecto de ML. La preparaci√≥n de datos puede tomar el 80% del tiempo en un proyecto de ML.\n",
    "\n",
    "### üßπ Pasos esenciales en la preparaci√≥n:\n",
    "\n",
    "1. **Limpieza de datos**: Manejar valores faltantes y outliers\n",
    "2. **Codificaci√≥n de variables categ√≥ricas**: Convertir texto a n√∫meros\n",
    "3. **Escalado de features**: Normalizar rangos de variables\n",
    "4. **Feature engineering**: Crear nuevas variables relevantes\n",
    "5. **Selecci√≥n de features**: Elegir las variables m√°s importantes\n",
    "6. **Divisi√≥n de datos**: Separar entrenamiento, validaci√≥n y prueba\n",
    "\n",
    "### üìà Vamos a trabajar con un dataset real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed370e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un dataset sint√©tico realista para demostraci√≥n\n",
    "np.random.seed(42)\n",
    "n_samples = 1000\n",
    "\n",
    "# Simular datos de empleados con salarios\n",
    "data = {\n",
    "    'edad': np.random.normal(35, 10, n_samples).clip(22, 65).astype(int),\n",
    "    'experiencia': np.random.exponential(5, n_samples).clip(0, 30).astype(int),\n",
    "    'educacion': np.random.choice(['Bachillerato', 'Universidad', 'Maestr√≠a', 'Doctorado'], \n",
    "                                 n_samples, p=[0.3, 0.5, 0.15, 0.05]),\n",
    "    'genero': np.random.choice(['M', 'F'], n_samples),\n",
    "    'departamento': np.random.choice(['IT', 'Marketing', 'Ventas', 'RRHH', 'Finanzas'], \n",
    "                                   n_samples, p=[0.3, 0.2, 0.25, 0.1, 0.15]),\n",
    "    'ciudad': np.random.choice(['Madrid', 'Barcelona', 'Valencia', 'Sevilla'], \n",
    "                              n_samples, p=[0.4, 0.3, 0.2, 0.1])\n",
    "}\n",
    "\n",
    "# Crear variable objetivo (salario) basada en otras variables\n",
    "base_salary = 30000\n",
    "salary = (base_salary + \n",
    "          data['edad'] * 500 + \n",
    "          data['experiencia'] * 1200 +\n",
    "          (data['educacion'] == 'Universidad').astype(int) * 5000 +\n",
    "          (data['educacion'] == 'Maestr√≠a').astype(int) * 12000 +\n",
    "          (data['educacion'] == 'Doctorado').astype(int) * 20000 +\n",
    "          (data['departamento'] == 'IT').astype(int) * 8000 +\n",
    "          np.random.normal(0, 5000, n_samples))\n",
    "\n",
    "data['salario'] = np.clip(salary, 25000, 120000).astype(int)\n",
    "\n",
    "# Introducir algunos valores faltantes de manera realista\n",
    "missing_indices = np.random.choice(n_samples, size=50, replace=False)\n",
    "data['experiencia'][missing_indices[:25]] = np.nan\n",
    "data['educacion'][missing_indices[25:]] = np.nan\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(\"üìä Dataset de empleados creado:\")\n",
    "print(f\"Tama√±o: {df.shape}\")\n",
    "print(\"\\nPrimeras filas:\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\nInformaci√≥n del dataset:\")\n",
    "print(df.info())\n",
    "\n",
    "print(\"\\nValores faltantes:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "print(\"\\nEstad√≠sticas descriptivas:\")\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53b9ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. MANEJO DE VALORES FALTANTES\n",
    "print(\"üîß 1. Manejo de valores faltantes\\n\")\n",
    "\n",
    "# Estrategias diferentes seg√∫n el tipo de variable\n",
    "df_clean = df.copy()\n",
    "\n",
    "# Para experiencia (num√©rica): imputar con la mediana\n",
    "df_clean['experiencia'].fillna(df_clean['experiencia'].median(), inplace=True)\n",
    "\n",
    "# Para educaci√≥n (categ√≥rica): imputar con la moda\n",
    "df_clean['educacion'].fillna(df_clean['educacion'].mode()[0], inplace=True)\n",
    "\n",
    "print(\"Valores faltantes despu√©s de la limpieza:\")\n",
    "print(df_clean.isnull().sum())\n",
    "\n",
    "# 2. CODIFICACI√ìN DE VARIABLES CATEG√ìRICAS\n",
    "print(\"\\nüî¢ 2. Codificaci√≥n de variables categ√≥ricas\\n\")\n",
    "\n",
    "# Label Encoding para variables ordinales (educaci√≥n tiene orden)\n",
    "label_encoder = LabelEncoder()\n",
    "df_clean['educacion_encoded'] = label_encoder.fit_transform(df_clean['educacion'])\n",
    "\n",
    "print(\"Mapeo de educaci√≥n:\")\n",
    "for i, label in enumerate(label_encoder.classes_):\n",
    "    print(f\"{label} -> {i}\")\n",
    "\n",
    "# One-Hot Encoding para variables nominales\n",
    "df_encoded = pd.get_dummies(df_clean, columns=['genero', 'departamento', 'ciudad'], prefix=['genero', 'dept', 'ciudad'])\n",
    "\n",
    "print(f\"\\nNuevas columnas despu√©s del One-Hot Encoding:\")\n",
    "print([col for col in df_encoded.columns if col not in df_clean.columns])\n",
    "\n",
    "# 3. ESCALADO DE FEATURES\n",
    "print(\"\\nüìè 3. Escalado de features\\n\")\n",
    "\n",
    "# Separar features num√©ricas\n",
    "numeric_features = ['edad', 'experiencia', 'educacion_encoded']\n",
    "X_numeric = df_encoded[numeric_features]\n",
    "\n",
    "# StandardScaler (media=0, std=1)\n",
    "scaler_standard = StandardScaler()\n",
    "X_scaled_standard = scaler_standard.fit_transform(X_numeric)\n",
    "\n",
    "# MinMaxScaler (rango 0-1)\n",
    "scaler_minmax = MinMaxScaler()\n",
    "X_scaled_minmax = scaler_minmax.fit_transform(X_numeric)\n",
    "\n",
    "# Comparar distribuciones\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "\n",
    "for i, feature in enumerate(numeric_features):\n",
    "    # Original\n",
    "    axes[0, i].hist(X_numeric.iloc[:, i], bins=30, alpha=0.7, color='blue')\n",
    "    axes[0, i].set_title(f'{feature} - Original')\n",
    "    \n",
    "    # Escalado est√°ndar\n",
    "    axes[1, i].hist(X_scaled_standard[:, i], bins=30, alpha=0.7, color='green')\n",
    "    axes[1, i].set_title(f'{feature} - StandardScaled')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Estad√≠sticas despu√©s del escalado est√°ndar:\")\n",
    "print(f\"Media: {X_scaled_standard.mean(axis=0)}\")\n",
    "print(f\"Desviaci√≥n est√°ndar: {X_scaled_standard.std(axis=0)}\")\n",
    "\n",
    "print(f\"\\nEscalado Min-Max - Rango: [{X_scaled_minmax.min():.2f}, {X_scaled_minmax.max():.2f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809e2e67",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. üéØ Algoritmos de Aprendizaje Supervisado\n",
    "\n",
    "El aprendizaje supervisado utiliza datos etiquetados para entrenar modelos que pueden hacer predicciones sobre nuevos datos.\n",
    "\n",
    "### üìä Regresi√≥n: Predecir valores num√©ricos\n",
    "\n",
    "Vamos a predecir el salario bas√°ndose en las caracter√≠sticas del empleado.\n",
    "\n",
    "#### Algoritmos de regresi√≥n que veremos:\n",
    "1. **Regresi√≥n Lineal**: Modelo m√°s simple, relaci√≥n lineal\n",
    "2. **Ridge Regression**: Regresi√≥n lineal con regularizaci√≥n L2\n",
    "3. **Random Forest**: Conjunto de √°rboles de decisi√≥n\n",
    "4. **Support Vector Regression**: M√°quinas de soporte vectorial\n",
    "\n",
    "### üè∑Ô∏è Clasificaci√≥n: Predecir categor√≠as\n",
    "\n",
    "Tambi√©n crearemos un problema de clasificaci√≥n: predecir el departamento bas√°ndose en las caracter√≠sticas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981fda91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROBLEMA DE REGRESI√ìN: Predecir salario\n",
    "print(\"üí∞ Problema de Regresi√≥n: Predicci√≥n de Salarios\\n\")\n",
    "\n",
    "# Preparar datos para regresi√≥n\n",
    "# Seleccionar features (excluir la variable objetivo 'salario')\n",
    "feature_columns = [col for col in df_encoded.columns if col not in ['salario', 'educacion']]\n",
    "X = df_encoded[feature_columns]\n",
    "y = df_encoded['salario']\n",
    "\n",
    "print(f\"Features utilizadas: {len(feature_columns)}\")\n",
    "print(f\"Primeras 5 features: {feature_columns[:5]}\")\n",
    "\n",
    "# Divisi√≥n de datos: 70% entrenamiento, 30% prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "print(f\"\\nTama√±o conjunto entrenamiento: {X_train.shape}\")\n",
    "print(f\"Tama√±o conjunto prueba: {X_test.shape}\")\n",
    "\n",
    "# Escalado de features (importante para algunos algoritmos)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# MODELOS DE REGRESI√ìN\n",
    "print(\"\\nüîß Entrenando modelos de regresi√≥n...\\n\")\n",
    "\n",
    "# 1. Regresi√≥n Lineal\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train_scaled, y_train)\n",
    "y_pred_lr = lr.predict(X_test_scaled)\n",
    "\n",
    "# 2. Ridge Regression (regularizaci√≥n L2)\n",
    "ridge = Ridge(alpha=1.0)\n",
    "ridge.fit(X_train_scaled, y_train)\n",
    "y_pred_ridge = ridge.predict(X_test_scaled)\n",
    "\n",
    "# 3. Random Forest (no necesita escalado)\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "\n",
    "# 4. Support Vector Regression\n",
    "svr = SVR(kernel='rbf', C=1000, gamma=0.1)\n",
    "svr.fit(X_train_scaled, y_train)\n",
    "y_pred_svr = svr.predict(X_test_scaled)\n",
    "\n",
    "# EVALUACI√ìN DE MODELOS\n",
    "print(\"üìä Evaluaci√≥n de modelos de regresi√≥n:\\n\")\n",
    "\n",
    "models = {\n",
    "    'Regresi√≥n Lineal': y_pred_lr,\n",
    "    'Ridge Regression': y_pred_ridge,\n",
    "    'Random Forest': y_pred_rf,\n",
    "    'SVR': y_pred_svr\n",
    "}\n",
    "\n",
    "results = []\n",
    "for name, predictions in models.items():\n",
    "    mse = mean_squared_error(y_test, predictions)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_test, predictions)\n",
    "    \n",
    "    results.append({\n",
    "        'Modelo': name,\n",
    "        'MSE': mse,\n",
    "        'RMSE': rmse,\n",
    "        'R¬≤ Score': r2\n",
    "    })\n",
    "    \n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  MSE: {mse:.2f}\")\n",
    "    print(f\"  RMSE: {rmse:.2f}\")\n",
    "    print(f\"  R¬≤ Score: {r2:.4f}\")\n",
    "    print()\n",
    "\n",
    "# Crear DataFrame con resultados\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"Resumen de resultados:\")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46c5ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VISUALIZACI√ìN DE RESULTADOS\n",
    "print(\"üìà Visualizaci√≥n de predicciones vs valores reales\\n\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('Predicciones vs Valores Reales - Modelos de Regresi√≥n', fontsize=16)\n",
    "\n",
    "models_plot = [\n",
    "    ('Regresi√≥n Lineal', y_pred_lr),\n",
    "    ('Ridge Regression', y_pred_ridge),\n",
    "    ('Random Forest', y_pred_rf),\n",
    "    ('SVR', y_pred_svr)\n",
    "]\n",
    "\n",
    "for i, (name, predictions) in enumerate(models_plot):\n",
    "    row = i // 2\n",
    "    col = i % 2\n",
    "    \n",
    "    axes[row, col].scatter(y_test, predictions, alpha=0.6)\n",
    "    axes[row, col].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "    axes[row, col].set_xlabel('Salario Real')\n",
    "    axes[row, col].set_ylabel('Salario Predicho')\n",
    "    axes[row, col].set_title(f'{name} (R¬≤ = {r2_score(y_test, predictions):.3f})')\n",
    "    axes[row, col].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Importancia de features (Random Forest)\n",
    "print(\"üéØ Importancia de Features (Random Forest):\\n\")\n",
    "\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_columns,\n",
    "    'importance': rf.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(feature_importance.head(10))\n",
    "\n",
    "# Visualizar importancia de features\n",
    "plt.figure(figsize=(10, 6))\n",
    "top_features = feature_importance.head(10)\n",
    "plt.barh(range(len(top_features)), top_features['importance'])\n",
    "plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "plt.xlabel('Importancia')\n",
    "plt.title('Top 10 Features m√°s Importantes (Random Forest)')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7faf7b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROBLEMA DE CLASIFICACI√ìN: Predecir departamento\n",
    "print(\"üè¢ Problema de Clasificaci√≥n: Predicci√≥n de Departamento\\n\")\n",
    "\n",
    "# Preparar datos para clasificaci√≥n\n",
    "X_class = df_encoded.drop(['departamento', 'salario'], axis=1)\n",
    "y_class = df_encoded['departamento']\n",
    "\n",
    "# Codificar variable objetivo\n",
    "le_dept = LabelEncoder()\n",
    "y_class_encoded = le_dept.fit_transform(y_class)\n",
    "\n",
    "print(f\"Clases a predecir: {le_dept.classes_}\")\n",
    "print(f\"Distribuci√≥n de clases:\")\n",
    "print(pd.Series(y_class).value_counts())\n",
    "\n",
    "# Divisi√≥n de datos\n",
    "X_train_c, X_test_c, y_train_c, y_test_c = train_test_split(\n",
    "    X_class, y_class_encoded, test_size=0.3, random_state=42, stratify=y_class_encoded\n",
    ")\n",
    "\n",
    "# Escalado para algoritmos que lo requieren\n",
    "X_train_c_scaled = scaler.fit_transform(X_train_c)\n",
    "X_test_c_scaled = scaler.transform(X_test_c)\n",
    "\n",
    "print(f\"\\nTama√±o conjunto entrenamiento: {X_train_c.shape}\")\n",
    "print(f\"Tama√±o conjunto prueba: {X_test_c.shape}\")\n",
    "\n",
    "# MODELOS DE CLASIFICACI√ìN\n",
    "print(\"\\nüîß Entrenando modelos de clasificaci√≥n...\\n\")\n",
    "\n",
    "# 1. Regresi√≥n Log√≠stica\n",
    "log_reg = LogisticRegression(random_state=42, max_iter=1000)\n",
    "log_reg.fit(X_train_c_scaled, y_train_c)\n",
    "y_pred_log = log_reg.predict(X_test_c_scaled)\n",
    "\n",
    "# 2. √Årbol de Decisi√≥n\n",
    "dt = DecisionTreeClassifier(random_state=42, max_depth=10)\n",
    "dt.fit(X_train_c, y_train_c)\n",
    "y_pred_dt = dt.predict(X_test_c)\n",
    "\n",
    "# 3. Random Forest\n",
    "rf_class = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_class.fit(X_train_c, y_train_c)\n",
    "y_pred_rf_class = rf_class.predict(X_test_c)\n",
    "\n",
    "# 4. Support Vector Machine\n",
    "svm = SVC(random_state=42, kernel='rbf')\n",
    "svm.fit(X_train_c_scaled, y_train_c)\n",
    "y_pred_svm = svm.predict(X_test_c_scaled)\n",
    "\n",
    "# 5. K-Nearest Neighbors\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train_c_scaled, y_train_c)\n",
    "y_pred_knn = knn.predict(X_test_c_scaled)\n",
    "\n",
    "# 6. Naive Bayes\n",
    "nb = GaussianNB()\n",
    "nb.fit(X_train_c_scaled, y_train_c)\n",
    "y_pred_nb = nb.predict(X_test_c_scaled)\n",
    "\n",
    "# EVALUACI√ìN DE MODELOS DE CLASIFICACI√ìN\n",
    "print(\"üìä Evaluaci√≥n de modelos de clasificaci√≥n:\\n\")\n",
    "\n",
    "classification_models = {\n",
    "    'Regresi√≥n Log√≠stica': y_pred_log,\n",
    "    '√Årbol de Decisi√≥n': y_pred_dt,\n",
    "    'Random Forest': y_pred_rf_class,\n",
    "    'SVM': y_pred_svm,\n",
    "    'K-NN': y_pred_knn,\n",
    "    'Naive Bayes': y_pred_nb\n",
    "}\n",
    "\n",
    "classification_results = []\n",
    "for name, predictions in classification_models.items():\n",
    "    accuracy = accuracy_score(y_test_c, predictions)\n",
    "    precision = precision_score(y_test_c, predictions, average='weighted')\n",
    "    recall = recall_score(y_test_c, predictions, average='weighted')\n",
    "    f1 = f1_score(y_test_c, predictions, average='weighted')\n",
    "    \n",
    "    classification_results.append({\n",
    "        'Modelo': name,\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-Score': f1\n",
    "    })\n",
    "    \n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"  Precision: {precision:.4f}\")\n",
    "    print(f\"  Recall: {recall:.4f}\")\n",
    "    print(f\"  F1-Score: {f1:.4f}\")\n",
    "    print()\n",
    "\n",
    "# Crear DataFrame con resultados\n",
    "classification_results_df = pd.DataFrame(classification_results)\n",
    "print(\"Resumen de resultados de clasificaci√≥n:\")\n",
    "print(classification_results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf904c2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. üîç Algoritmos de Aprendizaje No Supervisado\n",
    "\n",
    "El aprendizaje no supervisado encuentra patrones ocultos en datos sin etiquetas.\n",
    "\n",
    "### üéØ Principales t√©cnicas:\n",
    "\n",
    "1. **Clustering**: Agrupar datos similares\n",
    "   - K-Means: Agrupa en k clusters\n",
    "   - Clustering Jer√°rquico: Crea jerarqu√≠as de clusters\n",
    "\n",
    "2. **Reducci√≥n de Dimensionalidad**: Simplificar datos manteniendo informaci√≥n\n",
    "   - PCA (Principal Component Analysis): Componentes principales\n",
    "   - t-SNE: Visualizaci√≥n de datos de alta dimensi√≥n\n",
    "\n",
    "### üìä Vamos a aplicar estas t√©cnicas a nuestros datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5d78ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLUSTERING: Segmentaci√≥n de empleados\n",
    "print(\"üë• Clustering: Segmentaci√≥n de Empleados\\n\")\n",
    "\n",
    "# Usar solo variables num√©ricas para clustering\n",
    "numeric_cols = ['edad', 'experiencia', 'educacion_encoded', 'salario']\n",
    "X_cluster = df_encoded[numeric_cols].copy()\n",
    "\n",
    "# Escalado para clustering (importante para K-means)\n",
    "scaler_cluster = StandardScaler()\n",
    "X_cluster_scaled = scaler_cluster.fit_transform(X_cluster)\n",
    "\n",
    "# 1. K-MEANS CLUSTERING\n",
    "print(\"üéØ K-Means Clustering\\n\")\n",
    "\n",
    "# Encontrar el n√∫mero √≥ptimo de clusters usando el m√©todo del codo\n",
    "inertias = []\n",
    "k_range = range(2, 11)\n",
    "\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans.fit(X_cluster_scaled)\n",
    "    inertias.append(kmeans.inertia_)\n",
    "\n",
    "# Visualizar m√©todo del codo\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(k_range, inertias, 'bo-')\n",
    "plt.xlabel('N√∫mero de Clusters (k)')\n",
    "plt.ylabel('Inercia')\n",
    "plt.title('M√©todo del Codo para Determinar k √ìptimo')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Aplicar K-means con k=4\n",
    "k_optimal = 4\n",
    "kmeans = KMeans(n_clusters=k_optimal, random_state=42, n_init=10)\n",
    "clusters_kmeans = kmeans.fit_predict(X_cluster_scaled)\n",
    "\n",
    "# Agregar clusters al DataFrame\n",
    "df_clustered = df_encoded.copy()\n",
    "df_clustered['cluster_kmeans'] = clusters_kmeans\n",
    "\n",
    "print(f\"Distribuci√≥n de clusters K-Means (k={k_optimal}):\")\n",
    "print(pd.Series(clusters_kmeans).value_counts().sort_index())\n",
    "\n",
    "# 2. CLUSTERING JER√ÅRQUICO\n",
    "print(\"\\nüå≥ Clustering Jer√°rquico\\n\")\n",
    "\n",
    "hierarchical = AgglomerativeClustering(n_clusters=k_optimal)\n",
    "clusters_hierarchical = hierarchical.fit_predict(X_cluster_scaled)\n",
    "\n",
    "df_clustered['cluster_hierarchical'] = clusters_hierarchical\n",
    "\n",
    "print(f\"Distribuci√≥n de clusters Jer√°rquicos:\")\n",
    "print(pd.Series(clusters_hierarchical).value_counts().sort_index())\n",
    "\n",
    "# AN√ÅLISIS DE CLUSTERS\n",
    "print(\"\\nüìä An√°lisis de Clusters K-Means:\\n\")\n",
    "\n",
    "for cluster in range(k_optimal):\n",
    "    cluster_data = df_clustered[df_clustered['cluster_kmeans'] == cluster]\n",
    "    print(f\"Cluster {cluster} (n={len(cluster_data)}):\")\n",
    "    print(f\"  Edad promedio: {cluster_data['edad'].mean():.1f}\")\n",
    "    print(f\"  Experiencia promedio: {cluster_data['experiencia'].mean():.1f}\")\n",
    "    print(f\"  Salario promedio: {cluster_data['salario'].mean():.0f}\")\n",
    "    print(f\"  Departamento m√°s com√∫n: {cluster_data['departamento'].mode().iloc[0]}\")\n",
    "    print(f\"  Educaci√≥n m√°s com√∫n: {cluster_data['educacion'].mode().iloc[0]}\")\n",
    "    print()\n",
    "\n",
    "# VISUALIZACI√ìN DE CLUSTERS\n",
    "print(\"üìà Visualizaci√≥n de Clusters\\n\")\n",
    "\n",
    "# Reducir dimensionalidad para visualizar\n",
    "pca_viz = PCA(n_components=2, random_state=42)\n",
    "X_pca = pca_viz.fit_transform(X_cluster_scaled)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# K-Means clusters\n",
    "scatter1 = axes[0].scatter(X_pca[:, 0], X_pca[:, 1], c=clusters_kmeans, cmap='viridis', alpha=0.6)\n",
    "axes[0].set_title('K-Means Clustering (PCA)')\n",
    "axes[0].set_xlabel('Primera Componente Principal')\n",
    "axes[0].set_ylabel('Segunda Componente Principal')\n",
    "plt.colorbar(scatter1, ax=axes[0])\n",
    "\n",
    "# Hierarchical clusters\n",
    "scatter2 = axes[1].scatter(X_pca[:, 0], X_pca[:, 1], c=clusters_hierarchical, cmap='viridis', alpha=0.6)\n",
    "axes[1].set_title('Clustering Jer√°rquico (PCA)')\n",
    "axes[1].set_xlabel('Primera Componente Principal')\n",
    "axes[1].set_ylabel('Segunda Componente Principal')\n",
    "plt.colorbar(scatter2, ax=axes[1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248d5ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA - AN√ÅLISIS DE COMPONENTES PRINCIPALES\n",
    "print(\"üî¨ PCA - An√°lisis de Componentes Principales\\n\")\n",
    "\n",
    "# Aplicar PCA para entender la estructura de los datos\n",
    "pca_full = PCA()\n",
    "X_pca_full = pca_full.fit_transform(X_cluster_scaled)\n",
    "\n",
    "# Varianza explicada por cada componente\n",
    "explained_variance_ratio = pca_full.explained_variance_ratio_\n",
    "cumulative_variance = np.cumsum(explained_variance_ratio)\n",
    "\n",
    "print(\"Varianza explicada por componente:\")\n",
    "for i, var in enumerate(explained_variance_ratio):\n",
    "    print(f\"  Componente {i+1}: {var:.4f} ({var*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\nVarianza acumulada: {cumulative_variance}\")\n",
    "\n",
    "# Visualizar varianza explicada\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Varianza por componente\n",
    "axes[0].bar(range(1, len(explained_variance_ratio)+1), explained_variance_ratio)\n",
    "axes[0].set_xlabel('Componente Principal')\n",
    "axes[0].set_ylabel('Varianza Explicada')\n",
    "axes[0].set_title('Varianza Explicada por Componente')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Varianza acumulada\n",
    "axes[1].plot(range(1, len(cumulative_variance)+1), cumulative_variance, 'bo-')\n",
    "axes[1].axhline(y=0.95, color='r', linestyle='--', label='95% varianza')\n",
    "axes[1].set_xlabel('N√∫mero de Componentes')\n",
    "axes[1].set_ylabel('Varianza Acumulada')\n",
    "axes[1].set_title('Varianza Acumulada')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# PCA con 2 componentes para visualizaci√≥n\n",
    "pca_2d = PCA(n_components=2)\n",
    "X_pca_2d = pca_2d.fit_transform(X_cluster_scaled)\n",
    "\n",
    "print(f\"\\nCon 2 componentes explicamos {pca_2d.explained_variance_ratio_.sum():.3f} de la varianza\")\n",
    "\n",
    "# Interpretar componentes principales\n",
    "components_df = pd.DataFrame(\n",
    "    pca_2d.components_.T,\n",
    "    columns=['PC1', 'PC2'],\n",
    "    index=numeric_cols\n",
    ")\n",
    "\n",
    "print(\"\\nPesos de las variables en cada componente principal:\")\n",
    "print(components_df)\n",
    "\n",
    "# Visualizaci√≥n con informaci√≥n adicional\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# PCA coloreado por departamento\n",
    "dept_colors = df_encoded['departamento'].astype('category').cat.codes\n",
    "scatter1 = axes[0].scatter(X_pca_2d[:, 0], X_pca_2d[:, 1], c=dept_colors, cmap='tab10', alpha=0.6)\n",
    "axes[0].set_title('PCA - Coloreado por Departamento')\n",
    "axes[0].set_xlabel(f'PC1 ({pca_2d.explained_variance_ratio_[0]:.3f} varianza)')\n",
    "axes[0].set_ylabel(f'PC2 ({pca_2d.explained_variance_ratio_[1]:.3f} varianza)')\n",
    "\n",
    "# PCA coloreado por salario\n",
    "scatter2 = axes[1].scatter(X_pca_2d[:, 0], X_pca_2d[:, 1], c=df_encoded['salario'], cmap='viridis', alpha=0.6)\n",
    "axes[1].set_title('PCA - Coloreado por Salario')\n",
    "axes[1].set_xlabel(f'PC1 ({pca_2d.explained_variance_ratio_[0]:.3f} varianza)')\n",
    "axes[1].set_ylabel(f'PC2 ({pca_2d.explained_variance_ratio_[1]:.3f} varianza)')\n",
    "plt.colorbar(scatter2, ax=axes[1])\n",
    "\n",
    "# Biplot: variables y observaciones\n",
    "axes[2].scatter(X_pca_2d[:, 0], X_pca_2d[:, 1], alpha=0.3, s=30)\n",
    "\n",
    "# Vectores de variables\n",
    "for i, (var, pc1, pc2) in enumerate(zip(numeric_cols, components_df['PC1'], components_df['PC2'])):\n",
    "    axes[2].arrow(0, 0, pc1*3, pc2*3, head_width=0.1, head_length=0.1, fc='red', ec='red')\n",
    "    axes[2].text(pc1*3.2, pc2*3.2, var, fontsize=10, ha='center', va='center')\n",
    "\n",
    "axes[2].set_title('Biplot PCA')\n",
    "axes[2].set_xlabel(f'PC1 ({pca_2d.explained_variance_ratio_[0]:.3f} varianza)')\n",
    "axes[2].set_ylabel(f'PC2 ({pca_2d.explained_variance_ratio_[1]:.3f} varianza)')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43e4014",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. üìè Evaluaci√≥n y Validaci√≥n de Modelos\n",
    "\n",
    "La evaluaci√≥n correcta de modelos es crucial para evitar sobreajuste y asegurar que funcionen bien con datos nuevos.\n",
    "\n",
    "### üéØ T√©cnicas de validaci√≥n:\n",
    "\n",
    "1. **Validaci√≥n Cruzada**: Dividir datos en m√∫ltiples folds\n",
    "2. **Curvas de Aprendizaje**: Evaluar rendimiento vs tama√±o de datos\n",
    "3. **Curvas de Validaci√≥n**: Evaluar rendimiento vs hiperpar√°metros\n",
    "4. **M√©tricas espec√≠ficas**: Seg√∫n el tipo de problema\n",
    "\n",
    "### üìä Aplicaremos estas t√©cnicas a nuestros mejores modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d6b6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VALIDACI√ìN CRUZADA\n",
    "print(\"üîÑ Validaci√≥n Cruzada\\n\")\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, learning_curve, validation_curve\n",
    "\n",
    "# Seleccionar mejores modelos para evaluaci√≥n profunda\n",
    "best_models = {\n",
    "    'Random Forest (Regresi√≥n)': RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "    'Random Forest (Clasificaci√≥n)': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'Regresi√≥n Log√≠stica': LogisticRegression(random_state=42, max_iter=1000)\n",
    "}\n",
    "\n",
    "# VALIDACI√ìN CRUZADA PARA REGRESI√ìN\n",
    "print(\"üìä Validaci√≥n Cruzada - Problema de Regresi√≥n:\\n\")\n",
    "\n",
    "# Usar scoring apropiado para regresi√≥n\n",
    "cv_scores_reg = cross_val_score(\n",
    "    best_models['Random Forest (Regresi√≥n)'], \n",
    "    X_train_scaled, y_train, \n",
    "    cv=5, \n",
    "    scoring='r2'\n",
    ")\n",
    "\n",
    "print(f\"Random Forest (Regresi√≥n) - R¬≤ scores:\")\n",
    "print(f\"  Scores individuales: {cv_scores_reg}\")\n",
    "print(f\"  Promedio: {cv_scores_reg.mean():.4f}\")\n",
    "print(f\"  Desviaci√≥n est√°ndar: {cv_scores_reg.std():.4f}\")\n",
    "\n",
    "# VALIDACI√ìN CRUZADA PARA CLASIFICACI√ìN\n",
    "print(\"\\nüè∑Ô∏è Validaci√≥n Cruzada - Problema de Clasificaci√≥n:\\n\")\n",
    "\n",
    "cv_scores_rf_class = cross_val_score(\n",
    "    best_models['Random Forest (Clasificaci√≥n)'], \n",
    "    X_train_c, y_train_c, \n",
    "    cv=5, \n",
    "    scoring='accuracy'\n",
    ")\n",
    "\n",
    "cv_scores_log_reg = cross_val_score(\n",
    "    best_models['Regresi√≥n Log√≠stica'], \n",
    "    X_train_c_scaled, y_train_c, \n",
    "    cv=5, \n",
    "    scoring='accuracy'\n",
    ")\n",
    "\n",
    "print(f\"Random Forest (Clasificaci√≥n) - Accuracy scores:\")\n",
    "print(f\"  Scores individuales: {cv_scores_rf_class}\")\n",
    "print(f\"  Promedio: {cv_scores_rf_class.mean():.4f}\")\n",
    "print(f\"  Desviaci√≥n est√°ndar: {cv_scores_rf_class.std():.4f}\")\n",
    "\n",
    "print(f\"\\nRegresi√≥n Log√≠stica - Accuracy scores:\")\n",
    "print(f\"  Scores individuales: {cv_scores_log_reg}\")\n",
    "print(f\"  Promedio: {cv_scores_log_reg.mean():.4f}\")\n",
    "print(f\"  Desviaci√≥n est√°ndar: {cv_scores_log_reg.std():.4f}\")\n",
    "\n",
    "# CURVAS DE APRENDIZAJE\n",
    "print(\"\\nüìà Curvas de Aprendizaje\\n\")\n",
    "\n",
    "def plot_learning_curve(estimator, X, y, title, scoring='accuracy'):\n",
    "    train_sizes, train_scores, val_scores = learning_curve(\n",
    "        estimator, X, y, cv=5, train_sizes=np.linspace(0.1, 1.0, 10),\n",
    "        scoring=scoring, random_state=42\n",
    "    )\n",
    "    \n",
    "    train_mean = train_scores.mean(axis=1)\n",
    "    train_std = train_scores.std(axis=1)\n",
    "    val_mean = val_scores.mean(axis=1)\n",
    "    val_std = val_scores.std(axis=1)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_sizes, train_mean, 'o-', color='blue', label='Entrenamiento')\n",
    "    plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1, color='blue')\n",
    "    \n",
    "    plt.plot(train_sizes, val_mean, 'o-', color='red', label='Validaci√≥n')\n",
    "    plt.fill_between(train_sizes, val_mean - val_std, val_mean + val_std, alpha=0.1, color='red')\n",
    "    \n",
    "    plt.xlabel('Tama√±o del conjunto de entrenamiento')\n",
    "    plt.ylabel(f'Score ({scoring})')\n",
    "    plt.title(f'Curva de Aprendizaje - {title}')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    return train_sizes, train_scores, val_scores\n",
    "\n",
    "# Curva de aprendizaje para Random Forest (Clasificaci√≥n)\n",
    "plot_learning_curve(\n",
    "    RandomForestClassifier(n_estimators=50, random_state=42),\n",
    "    X_train_c, y_train_c,\n",
    "    'Random Forest (Clasificaci√≥n)',\n",
    "    scoring='accuracy'\n",
    ")\n",
    "\n",
    "# Curva de aprendizaje para Random Forest (Regresi√≥n)\n",
    "plot_learning_curve(\n",
    "    RandomForestRegressor(n_estimators=50, random_state=42),\n",
    "    X_train_scaled, y_train,\n",
    "    'Random Forest (Regresi√≥n)',\n",
    "    scoring='r2'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c234d1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. ‚öôÔ∏è Optimizaci√≥n de Hiperpar√°metros\n",
    "\n",
    "Los hiperpar√°metros son configuraciones que no se aprenden durante el entrenamiento pero afectan significativamente el rendimiento del modelo.\n",
    "\n",
    "### üîç T√©cnicas de optimizaci√≥n:\n",
    "\n",
    "1. **Grid Search**: B√∫squeda exhaustiva en una grilla de par√°metros\n",
    "2. **Random Search**: B√∫squeda aleatoria (m√°s eficiente para espacios grandes)\n",
    "3. **Validaci√≥n de hiperpar√°metros**: Evaluar el efecto de cada par√°metro\n",
    "\n",
    "### üéØ Optimizaremos nuestro mejor modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe4e9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIMIZACI√ìN DE HIPERPAR√ÅMETROS\n",
    "print(\"‚öôÔ∏è Optimizaci√≥n de Hiperpar√°metros\\n\")\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# GRID SEARCH para Random Forest (Clasificaci√≥n)\n",
    "print(\"üîç Grid Search - Random Forest (Clasificaci√≥n)\\n\")\n",
    "\n",
    "# Definir grilla de par√°metros (versi√≥n simplificada para rapidez)\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Grid Search\n",
    "rf_grid_search = GridSearchCV(\n",
    "    RandomForestClassifier(random_state=42),\n",
    "    param_grid_rf,\n",
    "    cv=3,  # 3-fold CV para rapidez\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,  # Usar todos los cores\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Entrenar con una muestra m√°s peque√±a para rapidez\n",
    "sample_size = 500\n",
    "sample_indices = np.random.choice(len(X_train_c), sample_size, replace=False)\n",
    "X_sample = X_train_c.iloc[sample_indices]\n",
    "y_sample = y_train_c[sample_indices]\n",
    "\n",
    "rf_grid_search.fit(X_sample, y_sample)\n",
    "\n",
    "print(f\"Mejores par√°metros encontrados:\")\n",
    "print(rf_grid_search.best_params_)\n",
    "print(f\"Mejor score: {rf_grid_search.best_score_:.4f}\")\n",
    "\n",
    "# RANDOM SEARCH (m√°s eficiente para espacios grandes)\n",
    "print(\"\\nüé≤ Random Search - Random Forest (Regresi√≥n)\\n\")\n",
    "\n",
    "param_dist_rf = {\n",
    "    'n_estimators': [50, 100, 150, 200, 250],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10, 15],\n",
    "    'min_samples_leaf': [1, 2, 4, 6],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "rf_random_search = RandomizedSearchCV(\n",
    "    RandomForestRegressor(random_state=42),\n",
    "    param_dist_rf,\n",
    "    n_iter=20,  # 20 combinaciones aleatorias\n",
    "    cv=3,\n",
    "    scoring='r2',\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Usar muestra para rapidez\n",
    "X_sample_reg = X_train_scaled[sample_indices]\n",
    "y_sample_reg = y_train.iloc[sample_indices]\n",
    "\n",
    "rf_random_search.fit(X_sample_reg, y_sample_reg)\n",
    "\n",
    "print(f\"Mejores par√°metros encontrados:\")\n",
    "print(rf_random_search.best_params_)\n",
    "print(f\"Mejor score: {rf_random_search.best_score_:.4f}\")\n",
    "\n",
    "# CURVAS DE VALIDACI√ìN\n",
    "print(\"\\nüìä Curvas de Validaci√≥n\\n\")\n",
    "\n",
    "# Evaluar el efecto del n√∫mero de estimadores\n",
    "param_range = [10, 25, 50, 100, 150, 200]\n",
    "\n",
    "train_scores, validation_scores = validation_curve(\n",
    "    RandomForestClassifier(random_state=42),\n",
    "    X_sample, y_sample,\n",
    "    param_name='n_estimators',\n",
    "    param_range=param_range,\n",
    "    cv=3,\n",
    "    scoring='accuracy'\n",
    ")\n",
    "\n",
    "# Visualizar curva de validaci√≥n\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "train_mean = train_scores.mean(axis=1)\n",
    "train_std = train_scores.std(axis=1)\n",
    "validation_mean = validation_scores.mean(axis=1)\n",
    "validation_std = validation_scores.std(axis=1)\n",
    "\n",
    "plt.plot(param_range, train_mean, 'o-', color='blue', label='Entrenamiento')\n",
    "plt.fill_between(param_range, train_mean - train_std, train_mean + train_std, alpha=0.1, color='blue')\n",
    "\n",
    "plt.plot(param_range, validation_mean, 'o-', color='red', label='Validaci√≥n')\n",
    "plt.fill_between(param_range, validation_mean - validation_std, validation_mean + validation_std, alpha=0.1, color='red')\n",
    "\n",
    "plt.xlabel('N√∫mero de Estimadores')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Curva de Validaci√≥n - n_estimators')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# COMPARAR MODELO OPTIMIZADO VS ORIGINAL\n",
    "print(\"\\nüèÜ Comparaci√≥n: Modelo Original vs Optimizado\\n\")\n",
    "\n",
    "# Modelo original\n",
    "rf_original = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_original.fit(X_train_c, y_train_c)\n",
    "y_pred_original = rf_original.predict(X_test_c)\n",
    "accuracy_original = accuracy_score(y_test_c, y_pred_original)\n",
    "\n",
    "# Modelo optimizado\n",
    "rf_optimized = RandomForestClassifier(**rf_grid_search.best_params_, random_state=42)\n",
    "rf_optimized.fit(X_train_c, y_train_c)\n",
    "y_pred_optimized = rf_optimized.predict(X_test_c)\n",
    "accuracy_optimized = accuracy_score(y_test_c, y_pred_optimized)\n",
    "\n",
    "print(f\"Modelo Original:\")\n",
    "print(f\"  Accuracy: {accuracy_original:.4f}\")\n",
    "\n",
    "print(f\"\\nModelo Optimizado:\")\n",
    "print(f\"  Accuracy: {accuracy_optimized:.4f}\")\n",
    "print(f\"  Mejora: {accuracy_optimized - accuracy_original:.4f}\")\n",
    "\n",
    "# Matriz de confusi√≥n para el modelo optimizado\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test_c, y_pred_optimized)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=le_dept.classes_, \n",
    "            yticklabels=le_dept.classes_)\n",
    "plt.title('Matriz de Confusi√≥n - Modelo Optimizado')\n",
    "plt.xlabel('Predicho')\n",
    "plt.ylabel('Real')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54dbb9cf",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. üé¨ Proyecto Integrador: Sistema de Recomendaciones de Pel√≠culas\n",
    "\n",
    "Aplicaremos todo lo aprendido en un proyecto real: construir un sistema de recomendaciones de pel√≠culas usando diferentes t√©cnicas de Machine Learning.\n",
    "\n",
    "### üéØ Objetivos del proyecto:\n",
    "1. Crear un dataset sint√©tico de pel√≠culas y ratings\n",
    "2. Implementar filtrado colaborativo\n",
    "3. Usar clustering para segmentar usuarios\n",
    "4. Crear un sistema h√≠brido de recomendaciones\n",
    "5. Evaluar y comparar diferentes enfoques\n",
    "\n",
    "### üìä Tipos de sistemas de recomendaci√≥n:\n",
    "- **Filtrado Colaborativo**: Recomienda bas√°ndose en usuarios similares\n",
    "- **Filtrado por Contenido**: Recomienda bas√°ndose en caracter√≠sticas del item\n",
    "- **Sistemas H√≠bridos**: Combina m√∫ltiples enfoques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343000e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROYECTO: SISTEMA DE RECOMENDACIONES DE PEL√çCULAS\n",
    "print(\"üé¨ Sistema de Recomendaciones de Pel√≠culas\\n\")\n",
    "\n",
    "# 1. CREAR DATASET SINT√âTICO DE PEL√çCULAS\n",
    "print(\"üìä 1. Creando dataset de pel√≠culas y ratings\\n\")\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# G√©neros de pel√≠culas\n",
    "genres = ['Acci√≥n', 'Comedia', 'Drama', 'Sci-Fi', 'Romance', 'Terror', 'Aventura', 'Animaci√≥n']\n",
    "\n",
    "# Crear dataset de pel√≠culas\n",
    "n_movies = 100\n",
    "movies_data = []\n",
    "\n",
    "for i in range(n_movies):\n",
    "    movie = {\n",
    "        'movie_id': i,\n",
    "        'title': f'Pel√≠cula_{i}',\n",
    "        'genre': np.random.choice(genres),\n",
    "        'year': np.random.randint(1990, 2024),\n",
    "        'duration': np.random.randint(80, 180),\n",
    "        'budget': np.random.randint(1, 200),  # Millones\n",
    "        'rating_avg': np.random.uniform(3.0, 9.0)\n",
    "    }\n",
    "    movies_data.append(movie)\n",
    "\n",
    "movies_df = pd.DataFrame(movies_data)\n",
    "\n",
    "print(f\"Dataset de pel√≠culas creado: {movies_df.shape}\")\n",
    "print(movies_df.head())\n",
    "\n",
    "# Crear dataset de usuarios\n",
    "n_users = 200\n",
    "users_data = []\n",
    "\n",
    "for i in range(n_users):\n",
    "    user = {\n",
    "        'user_id': i,\n",
    "        'age': np.random.randint(16, 70),\n",
    "        'gender': np.random.choice(['M', 'F']),\n",
    "        'preferred_genre': np.random.choice(genres)\n",
    "    }\n",
    "    users_data.append(user)\n",
    "\n",
    "users_df = pd.DataFrame(users_data)\n",
    "\n",
    "print(f\"\\nDataset de usuarios creado: {users_df.shape}\")\n",
    "print(users_df.head())\n",
    "\n",
    "# 2. GENERAR RATINGS REALISTAS\n",
    "print(\"\\n‚≠ê 2. Generando ratings de usuarios\\n\")\n",
    "\n",
    "ratings_data = []\n",
    "\n",
    "for user_id in range(n_users):\n",
    "    user = users_df.iloc[user_id]\n",
    "    n_ratings = np.random.randint(5, 30)  # Cada usuario califica 5-30 pel√≠culas\n",
    "    \n",
    "    movie_ids = np.random.choice(n_movies, n_ratings, replace=False)\n",
    "    \n",
    "    for movie_id in movie_ids:\n",
    "        movie = movies_df.iloc[movie_id]\n",
    "        \n",
    "        # Rating base de la pel√≠cula\n",
    "        base_rating = movie['rating_avg']\n",
    "        \n",
    "        # Ajuste por preferencia de g√©nero\n",
    "        if movie['genre'] == user['preferred_genre']:\n",
    "            genre_bonus = np.random.uniform(0.5, 1.5)\n",
    "        else:\n",
    "            genre_bonus = np.random.uniform(-0.5, 0.5)\n",
    "        \n",
    "        # A√±adir ruido\n",
    "        noise = np.random.normal(0, 0.8)\n",
    "        \n",
    "        # Rating final (1-10)\n",
    "        final_rating = np.clip(base_rating + genre_bonus + noise, 1, 10)\n",
    "        \n",
    "        ratings_data.append({\n",
    "            'user_id': user_id,\n",
    "            'movie_id': movie_id,\n",
    "            'rating': round(final_rating, 1)\n",
    "        })\n",
    "\n",
    "ratings_df = pd.DataFrame(ratings_data)\n",
    "\n",
    "print(f\"Dataset de ratings creado: {ratings_df.shape}\")\n",
    "print(f\"Promedio de ratings por usuario: {len(ratings_df) / n_users:.1f}\")\n",
    "print(f\"Rating promedio: {ratings_df['rating'].mean():.2f}\")\n",
    "\n",
    "print(\"\\nDistribuci√≥n de ratings:\")\n",
    "print(ratings_df['rating'].value_counts().sort_index())\n",
    "\n",
    "# 3. AN√ÅLISIS EXPLORATORIO\n",
    "print(\"\\nüîç 3. An√°lisis exploratorio\\n\")\n",
    "\n",
    "# Matriz usuario-pel√≠cula (sparse)\n",
    "user_movie_matrix = ratings_df.pivot(index='user_id', columns='movie_id', values='rating')\n",
    "print(f\"Matriz usuario-pel√≠cula: {user_movie_matrix.shape}\")\n",
    "print(f\"Densidad: {(~user_movie_matrix.isna()).sum().sum() / (user_movie_matrix.shape[0] * user_movie_matrix.shape[1]):.3f}\")\n",
    "\n",
    "# Rellenar NaN con 0 para algunos algoritmos\n",
    "user_movie_matrix_filled = user_movie_matrix.fillna(0)\n",
    "\n",
    "# Visualizaciones\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Distribuci√≥n de ratings\n",
    "axes[0, 0].hist(ratings_df['rating'], bins=20, edgecolor='black', alpha=0.7)\n",
    "axes[0, 0].set_title('Distribuci√≥n de Ratings')\n",
    "axes[0, 0].set_xlabel('Rating')\n",
    "axes[0, 0].set_ylabel('Frecuencia')\n",
    "\n",
    "# N√∫mero de ratings por usuario\n",
    "user_counts = ratings_df['user_id'].value_counts()\n",
    "axes[0, 1].hist(user_counts, bins=20, edgecolor='black', alpha=0.7)\n",
    "axes[0, 1].set_title('N√∫mero de Ratings por Usuario')\n",
    "axes[0, 1].set_xlabel('N√∫mero de Ratings')\n",
    "axes[0, 1].set_ylabel('N√∫mero de Usuarios')\n",
    "\n",
    "# Ratings por g√©nero\n",
    "genre_ratings = ratings_df.merge(movies_df, on='movie_id').groupby('genre')['rating'].mean().sort_values(ascending=False)\n",
    "axes[1, 0].bar(range(len(genre_ratings)), genre_ratings.values)\n",
    "axes[1, 0].set_title('Rating Promedio por G√©nero')\n",
    "axes[1, 0].set_xlabel('G√©nero')\n",
    "axes[1, 0].set_ylabel('Rating Promedio')\n",
    "axes[1, 0].set_xticks(range(len(genre_ratings)))\n",
    "axes[1, 0].set_xticklabels(genre_ratings.index, rotation=45)\n",
    "\n",
    "# Heatmap de correlaciones entre g√©neros\n",
    "genre_matrix = ratings_df.merge(movies_df, on='movie_id').pivot_table(\n",
    "    index='user_id', columns='genre', values='rating', aggfunc='mean'\n",
    ").fillna(0)\n",
    "\n",
    "correlation_matrix = genre_matrix.corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, ax=axes[1, 1])\n",
    "axes[1, 1].set_title('Correlaci√≥n entre G√©neros')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0774b727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. IMPLEMENTAR ALGORITMOS DE RECOMENDACI√ìN\n",
    "print(\"ü§ñ 4. Implementando algoritmos de recomendaci√≥n\\n\")\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# 4.1 FILTRADO COLABORATIVO BASADO EN USUARIOS\n",
    "print(\"üë• Filtrado Colaborativo Basado en Usuarios\\n\")\n",
    "\n",
    "def collaborative_filtering_users(user_id, user_movie_matrix, n_recommendations=5):\n",
    "    \"\"\"\n",
    "    Recomienda pel√≠culas bas√°ndose en usuarios similares\n",
    "    \"\"\"\n",
    "    # Calcular similitud entre usuarios\n",
    "    user_similarity = cosine_similarity(user_movie_matrix_filled)\n",
    "    \n",
    "    # Encontrar usuarios m√°s similares (excluyendo al propio usuario)\n",
    "    user_idx = user_id\n",
    "    similarities = user_similarity[user_idx]\n",
    "    similar_users = np.argsort(similarities)[::-1][1:11]  # Top 10 usuarios similares\n",
    "    \n",
    "    # Obtener pel√≠culas que el usuario no ha visto\n",
    "    user_ratings = user_movie_matrix.iloc[user_idx]\n",
    "    unwatched_movies = user_ratings[user_ratings.isna()].index\n",
    "    \n",
    "    # Calcular scores para pel√≠culas no vistas\n",
    "    movie_scores = {}\n",
    "    \n",
    "    for movie_id in unwatched_movies:\n",
    "        weighted_sum = 0\n",
    "        similarity_sum = 0\n",
    "        \n",
    "        for similar_user in similar_users:\n",
    "            if not pd.isna(user_movie_matrix.iloc[similar_user, movie_id]):\n",
    "                rating = user_movie_matrix.iloc[similar_user, movie_id]\n",
    "                similarity = similarities[similar_user]\n",
    "                weighted_sum += rating * similarity\n",
    "                similarity_sum += similarity\n",
    "        \n",
    "        if similarity_sum > 0:\n",
    "            movie_scores[movie_id] = weighted_sum / similarity_sum\n",
    "    \n",
    "    # Ordenar y devolver top recomendaciones\n",
    "    recommended_movies = sorted(movie_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    return recommended_movies[:n_recommendations]\n",
    "\n",
    "# Ejemplo de recomendaci√≥n\n",
    "user_test = 0\n",
    "recommendations_user = collaborative_filtering_users(user_test, user_movie_matrix)\n",
    "\n",
    "print(f\"Recomendaciones para Usuario {user_test}:\")\n",
    "for movie_id, score in recommendations_user:\n",
    "    movie_title = movies_df.iloc[movie_id]['title']\n",
    "    movie_genre = movies_df.iloc[movie_id]['genre']\n",
    "    print(f\"  {movie_title} ({movie_genre}) - Score: {score:.2f}\")\n",
    "\n",
    "# 4.2 FILTRADO COLABORATIVO BASADO EN ITEMS\n",
    "print(f\"\\nüé¨ Filtrado Colaborativo Basado en Items\\n\")\n",
    "\n",
    "def collaborative_filtering_items(user_id, user_movie_matrix, n_recommendations=5):\n",
    "    \"\"\"\n",
    "    Recomienda pel√≠culas bas√°ndose en similitud entre pel√≠culas\n",
    "    \"\"\"\n",
    "    # Transponer para tener pel√≠culas como filas\n",
    "    movie_user_matrix = user_movie_matrix_filled.T\n",
    "    \n",
    "    # Calcular similitud entre pel√≠culas\n",
    "    movie_similarity = cosine_similarity(movie_user_matrix)\n",
    "    \n",
    "    # Obtener pel√≠culas que el usuario ha calificado bien (rating >= 7)\n",
    "    user_ratings = user_movie_matrix.iloc[user_id]\n",
    "    liked_movies = user_ratings[user_ratings >= 7].index\n",
    "    \n",
    "    # Pel√≠culas no vistas\n",
    "    unwatched_movies = user_ratings[user_ratings.isna()].index\n",
    "    \n",
    "    # Calcular scores para pel√≠culas no vistas\n",
    "    movie_scores = {}\n",
    "    \n",
    "    for movie_id in unwatched_movies:\n",
    "        weighted_sum = 0\n",
    "        similarity_sum = 0\n",
    "        \n",
    "        for liked_movie in liked_movies:\n",
    "            similarity = movie_similarity[movie_id, liked_movie]\n",
    "            rating = user_ratings[liked_movie]\n",
    "            weighted_sum += rating * similarity\n",
    "            similarity_sum += similarity\n",
    "        \n",
    "        if similarity_sum > 0:\n",
    "            movie_scores[movie_id] = weighted_sum / similarity_sum\n",
    "    \n",
    "    # Ordenar y devolver top recomendaciones\n",
    "    recommended_movies = sorted(movie_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    return recommended_movies[:n_recommendations]\n",
    "\n",
    "recommendations_item = collaborative_filtering_items(user_test, user_movie_matrix)\n",
    "\n",
    "print(f\"Recomendaciones basadas en items para Usuario {user_test}:\")\n",
    "for movie_id, score in recommendations_item:\n",
    "    movie_title = movies_df.iloc[movie_id]['title']\n",
    "    movie_genre = movies_df.iloc[movie_id]['genre']\n",
    "    print(f\"  {movie_title} ({movie_genre}) - Score: {score:.2f}\")\n",
    "\n",
    "# 4.3 MATRIX FACTORIZATION (SVD)\n",
    "print(f\"\\nüî¢ Matrix Factorization (SVD)\\n\")\n",
    "\n",
    "# Aplicar SVD\n",
    "n_components = 20\n",
    "svd = TruncatedSVD(n_components=n_components, random_state=42)\n",
    "user_factors = svd.fit_transform(user_movie_matrix_filled)\n",
    "movie_factors = svd.components_.T\n",
    "\n",
    "print(f\"Factorizaci√≥n completada:\")\n",
    "print(f\"  Usuarios: {user_factors.shape}\")\n",
    "print(f\"  Pel√≠culas: {movie_factors.shape}\")\n",
    "print(f\"  Varianza explicada: {svd.explained_variance_ratio_.sum():.3f}\")\n",
    "\n",
    "def svd_recommendations(user_id, user_factors, movie_factors, user_movie_matrix, n_recommendations=5):\n",
    "    \"\"\"\n",
    "    Recomienda usando factorizaci√≥n matricial\n",
    "    \"\"\"\n",
    "    # Calcular scores predichos\n",
    "    user_vector = user_factors[user_id]\n",
    "    predicted_ratings = np.dot(user_vector, movie_factors.T)\n",
    "    \n",
    "    # Pel√≠culas no vistas\n",
    "    user_ratings = user_movie_matrix.iloc[user_id]\n",
    "    unwatched_movies = user_ratings[user_ratings.isna()].index\n",
    "    \n",
    "    # Obtener scores para pel√≠culas no vistas\n",
    "    movie_scores = [(movie_id, predicted_ratings[movie_id]) for movie_id in unwatched_movies]\n",
    "    \n",
    "    # Ordenar y devolver top recomendaciones\n",
    "    recommended_movies = sorted(movie_scores, key=lambda x: x[1], reverse=True)\n",
    "    return recommended_movies[:n_recommendations]\n",
    "\n",
    "recommendations_svd = svd_recommendations(user_test, user_factors, movie_factors, user_movie_matrix)\n",
    "\n",
    "print(f\"Recomendaciones SVD para Usuario {user_test}:\")\n",
    "for movie_id, score in recommendations_svd:\n",
    "    movie_title = movies_df.iloc[movie_id]['title']\n",
    "    movie_genre = movies_df.iloc[movie_id]['genre']\n",
    "    print(f\"  {movie_title} ({movie_genre}) - Score: {score:.2f}\")\n",
    "\n",
    "# 4.4 SISTEMA H√çBRIDO\n",
    "print(f\"\\nüîÑ Sistema H√≠brido\\n\")\n",
    "\n",
    "def hybrid_recommendations(user_id, user_movie_matrix, user_factors, movie_factors, n_recommendations=5):\n",
    "    \"\"\"\n",
    "    Combina m√∫ltiples enfoques de recomendaci√≥n\n",
    "    \"\"\"\n",
    "    # Obtener recomendaciones de cada m√©todo\n",
    "    rec_user = collaborative_filtering_users(user_id, user_movie_matrix, 10)\n",
    "    rec_item = collaborative_filtering_items(user_id, user_movie_matrix, 10)\n",
    "    rec_svd = svd_recommendations(user_id, user_factors, movie_factors, user_movie_matrix, 10)\n",
    "    \n",
    "    # Combinar scores (promedio ponderado)\n",
    "    combined_scores = {}\n",
    "    \n",
    "    # Peso para cada m√©todo\n",
    "    weights = {'user': 0.3, 'item': 0.3, 'svd': 0.4}\n",
    "    \n",
    "    for movie_id, score in rec_user:\n",
    "        combined_scores[movie_id] = combined_scores.get(movie_id, 0) + score * weights['user']\n",
    "    \n",
    "    for movie_id, score in rec_item:\n",
    "        combined_scores[movie_id] = combined_scores.get(movie_id, 0) + score * weights['item']\n",
    "    \n",
    "    for movie_id, score in rec_svd:\n",
    "        combined_scores[movie_id] = combined_scores.get(movie_id, 0) + score * weights['svd']\n",
    "    \n",
    "    # Ordenar y devolver top recomendaciones\n",
    "    recommended_movies = sorted(combined_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    return recommended_movies[:n_recommendations]\n",
    "\n",
    "recommendations_hybrid = hybrid_recommendations(user_test, user_movie_matrix, user_factors, movie_factors)\n",
    "\n",
    "print(f\"Recomendaciones H√≠bridas para Usuario {user_test}:\")\n",
    "for movie_id, score in recommendations_hybrid:\n",
    "    movie_title = movies_df.iloc[movie_id]['title']\n",
    "    movie_genre = movies_df.iloc[movie_id]['genre']\n",
    "    print(f\"  {movie_title} ({movie_genre}) - Score: {score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e55e94",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ Conclusiones y Pr√≥ximos Pasos\n",
    "\n",
    "### üìä Lo que hemos aprendido en este m√≥dulo:\n",
    "\n",
    "1. **‚úÖ Fundamentos del Machine Learning**\n",
    "   - Tipos de aprendizaje (supervisado, no supervisado, por refuerzo)\n",
    "   - Flujo de trabajo en proyectos de ML\n",
    "   - Preparaci√≥n y preprocesamiento de datos\n",
    "\n",
    "2. **‚úÖ Algoritmos de Aprendizaje Supervisado**\n",
    "   - Regresi√≥n: Linear, Ridge, Random Forest, SVR\n",
    "   - Clasificaci√≥n: Log√≠stica, √Årboles, Random Forest, SVM, K-NN, Naive Bayes\n",
    "   - M√©tricas de evaluaci√≥n apropiadas para cada tipo\n",
    "\n",
    "3. **‚úÖ Algoritmos de Aprendizaje No Supervisado**\n",
    "   - Clustering: K-Means, Clustering Jer√°rquico\n",
    "   - Reducci√≥n de dimensionalidad: PCA\n",
    "   - An√°lisis de componentes principales\n",
    "\n",
    "4. **‚úÖ Evaluaci√≥n y Validaci√≥n**\n",
    "   - Validaci√≥n cruzada\n",
    "   - Curvas de aprendizaje\n",
    "   - Detecci√≥n de sobreajuste y subajuste\n",
    "\n",
    "5. **‚úÖ Optimizaci√≥n de Hiperpar√°metros**\n",
    "   - Grid Search exhaustivo\n",
    "   - Random Search eficiente\n",
    "   - Curvas de validaci√≥n\n",
    "\n",
    "6. **‚úÖ Proyecto Integrador**\n",
    "   - Sistema de recomendaciones completo\n",
    "   - Filtrado colaborativo (usuarios e items)\n",
    "   - Matrix Factorization (SVD)\n",
    "   - Sistema h√≠brido\n",
    "\n",
    "### üöÄ Pr√≥ximo m√≥dulo: Deep Learning\n",
    "\n",
    "En el **M√≥dulo 7** exploraremos:\n",
    "- Redes neuronales artificiales\n",
    "- TensorFlow/Keras para Deep Learning\n",
    "- CNNs para visi√≥n por computadora\n",
    "- RNNs para secuencias y texto\n",
    "- Transfer Learning\n",
    "- Proyectos avanzados de Deep Learning\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Ejercicios Finales del M√≥dulo\n",
    "\n",
    "### Ejercicio 1: An√°lisis comparativo de algoritmos\n",
    "Usando el dataset de empleados:\n",
    "1. Implementa y compara todos los algoritmos de clasificaci√≥n\n",
    "2. Usa validaci√≥n cruzada para evaluar cada uno\n",
    "3. Crea visualizaciones comparativas\n",
    "4. Selecciona el mejor modelo y justifica tu elecci√≥n\n",
    "\n",
    "### Ejercicio 2: Sistema de recomendaciones personalizado\n",
    "1. Modifica el sistema de recomendaciones para incluir informaci√≥n demogr√°fica\n",
    "2. Implementa un filtro por g√©nero de pel√≠cula preferido\n",
    "3. A√±ade penalizaci√≥n por antig√ºedad de la pel√≠cula\n",
    "4. Eval√∫a la calidad de las recomendaciones\n",
    "\n",
    "### Ejercicio 3: Optimizaci√≥n avanzada\n",
    "1. Implementa b√∫squeda bayesiana de hiperpar√°metros\n",
    "2. Usa ensemble methods (Voting, Bagging, Boosting)\n",
    "3. Compara rendimiento vs tiempo de entrenamiento\n",
    "4. Crea un pipeline completo de ML\n",
    "\n",
    "### Ejercicio 4: An√°lisis de clustering avanzado\n",
    "1. Implementa DBSCAN para clustering\n",
    "2. Compara con K-Means y clustering jer√°rquico\n",
    "3. Usa t-SNE para visualizaci√≥n\n",
    "4. Interpreta y valida los clusters encontrados\n",
    "\n",
    "---\n",
    "\n",
    "## üéâ ¬°Felicitaciones! Has completado el M√≥dulo 6\n",
    "\n",
    "### üìö Resumen de habilidades adquiridas:\n",
    "\n",
    "- **üîß Preparaci√≥n de datos**: Limpieza, escalado, codificaci√≥n\n",
    "- **üéØ Modelado supervisado**: Regresi√≥n y clasificaci√≥n\n",
    "- **üîç An√°lisis no supervisado**: Clustering y reducci√≥n dimensional\n",
    "- **üìä Evaluaci√≥n rigurosa**: Validaci√≥n cruzada y m√©tricas\n",
    "- **‚öôÔ∏è Optimizaci√≥n**: B√∫squeda de hiperpar√°metros\n",
    "- **ü§ñ Proyectos reales**: Sistema de recomendaciones\n",
    "\n",
    "### üìñ Recursos adicionales recomendados:\n",
    "\n",
    "- **Libros**: \n",
    "  - \"Hands-On Machine Learning\" by Aur√©lien G√©ron\n",
    "  - \"The Elements of Statistical Learning\" by Hastie, Tibshirani & Friedman\n",
    "- **Documentaci√≥n**: Scikit-learn, XGBoost, LightGBM\n",
    "- **Pr√°ctica**: Kaggle competitions, OpenML datasets\n",
    "- **Comunidad**: Machine Learning subreddit, Stack Overflow\n",
    "\n",
    "¬°Est√°s listo para Deep Learning! üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13878eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ü§ñ Machine Learning con Python - M√≥dulo 6\n",
    "\n",
    "## Bienvenido al M√≥dulo de Machine Learning\n",
    "\n",
    "### üìö Contenido del M√≥dulo 6:\n",
    "1. **Introducci√≥n al Machine Learning**\n",
    "2. **Preparaci√≥n de datos para ML**\n",
    "3. **Algoritmos de aprendizaje supervisado**\n",
    "4. **Algoritmos de aprendizaje no supervisado**\n",
    "5. **Evaluaci√≥n y validaci√≥n de modelos**\n",
    "6. **Optimizaci√≥n de hiperpar√°metros**\n",
    "7. **Proyecto: Sistema de recomendaciones**\n",
    "\n",
    "### üéØ Objetivos de Aprendizaje:\n",
    "- Entender los conceptos fundamentales del ML\n",
    "- Dominar scikit-learn para construir modelos\n",
    "- Implementar algoritmos de clasificaci√≥n y regresi√≥n\n",
    "- Realizar clustering y an√°lisis de componentes principales\n",
    "- Evaluar y optimizar modelos de ML\n",
    "- Aplicar t√©cnicas de feature engineering\n",
    "- Construir un sistema de recomendaciones completo\n",
    "\n",
    "### üõ†Ô∏è Tecnolog√≠as y bibliotecas:\n",
    "- **Scikit-learn**: Framework principal de ML\n",
    "- **XGBoost**: Gradient boosting avanzado\n",
    "- **Pandas & NumPy**: Manipulaci√≥n de datos\n",
    "- **Matplotlib & Seaborn**: Visualizaci√≥n\n",
    "- **Plotly**: Visualizaciones interactivas\n",
    "- **Joblib**: Persistencia de modelos\n",
    "- **SHAP**: Explicabilidad de modelos\n",
    "\n",
    "---\n",
    "\n",
    "## 1. üß† Introducci√≥n al Machine Learning\n",
    "\n",
    "El Machine Learning es una rama de la inteligencia artificial que permite a las computadoras aprender y hacer predicciones o decisiones sin ser expl√≠citamente programadas para cada tarea espec√≠fica.\n",
    "\n",
    "### üåü Tipos de Machine Learning:\n",
    "\n",
    "1. **Aprendizaje Supervisado**: Aprendemos de datos etiquetados\n",
    "   - Clasificaci√≥n: Predecir categor√≠as\n",
    "   - Regresi√≥n: Predecir valores num√©ricos\n",
    "\n",
    "2. **Aprendizaje No Supervisado**: Encontrar patrones sin etiquetas\n",
    "   - Clustering: Agrupar datos similares\n",
    "   - Reducci√≥n de dimensionalidad: Simplificar datos\n",
    "\n",
    "3. **Aprendizaje por Refuerzo**: Aprender a trav√©s de recompensas\n",
    "   - Agentes que interact√∫an con un entorno\n",
    "\n",
    "### üîÑ Flujo de trabajo t√≠pico en ML:\n",
    "\n",
    "1. **Definici√≥n del problema**: ¬øQu√© queremos predecir?\n",
    "2. **Recolecci√≥n de datos**: Obtener datos relevantes\n",
    "3. **Exploraci√≥n y limpieza**: Entender y preparar los datos\n",
    "4. **Feature engineering**: Crear variables relevantes\n",
    "5. **Selecci√≥n del modelo**: Elegir algoritmos apropiados\n",
    "6. **Entrenamiento**: Ajustar el modelo a los datos\n",
    "7. **Evaluaci√≥n**: Medir el rendimiento del modelo\n",
    "8. **Optimizaci√≥n**: Mejorar el modelo\n",
    "9. **Despliegue**: Poner el modelo en producci√≥n\n",
    "\n",
    "---\n",
    "\n",
    "## 2. üìä Preparaci√≥n de datos para ML\n",
    "\n",
    "La calidad de los datos determina el √©xito de cualquier proyecto de ML. La preparaci√≥n de datos puede tomar el 80% del tiempo en un proyecto de ML."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
