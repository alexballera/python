{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìä An√°lisis de Datos con Python - M√≥dulo 5\n",
    "\n",
    "## Bienvenido al M√≥dulo de An√°lisis de Datos\n",
    "\n",
    "### üìö Contenido del M√≥dulo 5:\n",
    "1. **Introducci√≥n al an√°lisis de datos**\n",
    "2. **Manipulaci√≥n de datos con Pandas**\n",
    "3. **An√°lisis exploratorio de datos (EDA)**\n",
    "4. **Manipulaci√≥n avanzada de datos**\n",
    "5. **Visualizaci√≥n avanzada**\n",
    "6. **Extracci√≥n de datos**\n",
    "7. **Proyecto: An√°lisis de datos de COVID-19**\n",
    "\n",
    "### üéØ Objetivos de Aprendizaje:\n",
    "- Dominar la manipulaci√≥n de datos con Pandas\n",
    "- Realizar an√°lisis exploratorio efectivo\n",
    "- Crear visualizaciones impactantes\n",
    "- Extraer datos de diversas fuentes\n",
    "- Aplicar t√©cnicas de an√°lisis a datos reales\n",
    "- Construir dashboards interactivos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. üìà Introducci√≥n al An√°lisis de Datos\n",
    "\n",
    "El an√°lisis de datos es el proceso de inspeccionar, limpiar, transformar y modelar datos con el objetivo de descubrir informaci√≥n √∫til, informar conclusiones y apoyar la toma de decisiones.\n",
    "\n",
    "### üåü ¬øPor qu√© Python para an√°lisis de datos?\n",
    "\n",
    "Python se ha convertido en el lenguaje preferido para an√°lisis de datos por varias razones:\n",
    "\n",
    "- **Ecosistema rico**: Pandas, NumPy, Matplotlib, Scikit-learn\n",
    "- **Sintaxis clara y legible**: Facilita el desarrollo y mantenimiento\n",
    "- **Comunidad activa**: Abundantes recursos y bibliotecas\n",
    "- **Versatilidad**: Desde an√°lisis b√°sico hasta machine learning avanzado\n",
    "- **Integraci√≥n**: Funciona bien con otras herramientas y sistemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.3' requires the ipykernel package.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
      "\u001b[1;31mOr install 'ipykernel' using the command: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Importar las bibliotecas principales para an√°lisis de datos\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configuraci√≥n para visualizaciones m√°s atractivas\n",
    "plt.style.use('ggplot')\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Mostrar versiones\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Matplotlib version: {plt.__version__}\")\n",
    "print(f\"Seaborn version: {sns.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîÑ Flujo de trabajo t√≠pico en an√°lisis de datos\n",
    "\n",
    "1. **Obtenci√≥n de datos**: Recolectar datos de diversas fuentes\n",
    "2. **Limpieza de datos**: Manejar valores faltantes, duplicados, errores\n",
    "3. **Exploraci√≥n**: An√°lisis estad√≠stico y visualizaci√≥n inicial\n",
    "4. **Transformaci√≥n**: Crear nuevas variables, normalizar, agregar\n",
    "5. **An√°lisis**: Aplicar t√©cnicas estad√≠sticas y modelos\n",
    "6. **Visualizaci√≥n**: Crear gr√°ficos y dashboards informativos\n",
    "7. **Comunicaci√≥n**: Presentar hallazgos y conclusiones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. üêº Manipulaci√≥n de datos con Pandas\n",
    "\n",
    "Pandas es la biblioteca m√°s importante para manipulaci√≥n y an√°lisis de datos en Python. Proporciona estructuras de datos flexibles y herramientas para trabajar con datos estructurados.\n",
    "\n",
    "### üìã Estructuras de datos principales:\n",
    "\n",
    "1. **Series**: Array unidimensional etiquetado\n",
    "2. **DataFrame**: Tabla bidimensional con columnas potencialmente de diferentes tipos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear una Series\n",
    "s = pd.Series([1, 3, 5, 7, 9], index=['a', 'b', 'c', 'd', 'e'])\n",
    "print(\"Series de Pandas:\")\n",
    "print(s)\n",
    "print(\"\\nTipo de datos:\", type(s))\n",
    "\n",
    "# Crear un DataFrame\n",
    "data = {\n",
    "    'Nombre': ['Ana', 'Carlos', 'Mar√≠a', 'Pedro', 'Laura'],\n",
    "    'Edad': [28, 34, 29, 42, 31],\n",
    "    'Ciudad': ['Madrid', 'Barcelona', 'Sevilla', 'Valencia', 'Bilbao'],\n",
    "    'Puntuaci√≥n': [85, 92, 78, 96, 89]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(\"\\nDataFrame de Pandas:\")\n",
    "print(df)\n",
    "\n",
    "# Informaci√≥n b√°sica del DataFrame\n",
    "print(\"\\nInformaci√≥n del DataFrame:\")\n",
    "print(df.info())\n",
    "\n",
    "# Estad√≠sticas descriptivas\n",
    "print(\"\\nEstad√≠sticas descriptivas:\")\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üì• Carga de datos desde diferentes fuentes\n",
    "\n",
    "Pandas puede leer datos de m√∫ltiples formatos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de carga de datos (comentado para evitar errores)\n",
    "\n",
    "# CSV\n",
    "# df_csv = pd.read_csv('datos/ejemplo.csv')\n",
    "\n",
    "# Excel\n",
    "# df_excel = pd.read_excel('datos/ejemplo.xlsx', sheet_name='Hoja1')\n",
    "\n",
    "# JSON\n",
    "# df_json = pd.read_json('datos/ejemplo.json')\n",
    "\n",
    "# SQL (requiere SQLAlchemy)\n",
    "# from sqlalchemy import create_engine\n",
    "# engine = create_engine('sqlite:///ejemplo.db')\n",
    "# df_sql = pd.read_sql('SELECT * FROM tabla', engine)\n",
    "\n",
    "# Crear un DataFrame de ejemplo para continuar\n",
    "df_ventas = pd.DataFrame({\n",
    "    'Fecha': pd.date_range(start='2023-01-01', periods=10),\n",
    "    'Producto': ['A', 'B', 'A', 'C', 'B', 'A', 'C', 'B', 'A', 'C'],\n",
    "    'Cantidad': [10, 5, 15, 8, 12, 20, 7, 9, 14, 11],\n",
    "    'Precio': [100, 200, 100, 150, 200, 100, 150, 200, 100, 150],\n",
    "    'Vendedor': ['Juan', 'Mar√≠a', 'Juan', 'Pedro', 'Mar√≠a', 'Juan', 'Pedro', 'Mar√≠a', 'Juan', 'Pedro']\n",
    "})\n",
    "\n",
    "print(\"DataFrame de ventas:\")\n",
    "print(df_ventas.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üßπ Limpieza y transformaci√≥n de datos\n",
    "\n",
    "La limpieza de datos es una parte crucial del an√°lisis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un DataFrame con problemas comunes\n",
    "df_sucio = pd.DataFrame({\n",
    "    'A': [1, 2, np.nan, 4, 5],\n",
    "    'B': [5, 6, 7, np.nan, 9],\n",
    "    'C': ['a', 'b', 'c', 'd', np.nan]\n",
    "})\n",
    "\n",
    "print(\"DataFrame con valores faltantes:\")\n",
    "print(df_sucio)\n",
    "\n",
    "# Detectar valores faltantes\n",
    "print(\"\\nValores faltantes por columna:\")\n",
    "print(df_sucio.isna().sum())\n",
    "\n",
    "# Rellenar valores faltantes\n",
    "df_limpio1 = df_sucio.fillna({'A': 0, 'B': 0, 'C': 'desconocido'})\n",
    "print(\"\\nDataFrame con valores faltantes rellenados:\")\n",
    "print(df_limpio1)\n",
    "\n",
    "# Eliminar filas con valores faltantes\n",
    "df_limpio2 = df_sucio.dropna()\n",
    "print(\"\\nDataFrame con filas con valores faltantes eliminadas:\")\n",
    "print(df_limpio2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç Filtrado y selecci√≥n de datos\n",
    "\n",
    "Pandas ofrece m√∫ltiples formas de seleccionar y filtrar datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Volvemos al DataFrame de ventas\n",
    "print(\"DataFrame de ventas:\")\n",
    "print(df_ventas)\n",
    "\n",
    "# Selecci√≥n de columnas\n",
    "print(\"\\nSelecci√≥n de columnas:\")\n",
    "print(df_ventas[['Producto', 'Cantidad']])\n",
    "\n",
    "# Filtrado con condiciones\n",
    "print(\"\\nVentas del producto A:\")\n",
    "print(df_ventas[df_ventas['Producto'] == 'A'])\n",
    "\n",
    "# Filtrado con m√∫ltiples condiciones\n",
    "print(\"\\nVentas del producto A con cantidad > 10:\")\n",
    "print(df_ventas[(df_ventas['Producto'] == 'A') & (df_ventas['Cantidad'] > 10)])\n",
    "\n",
    "# Selecci√≥n por √≠ndice con .loc\n",
    "print(\"\\nSelecci√≥n por √≠ndice con .loc:\")\n",
    "print(df_ventas.loc[2:4, ['Fecha', 'Producto', 'Cantidad']])\n",
    "\n",
    "# Selecci√≥n por posici√≥n con .iloc\n",
    "print(\"\\nSelecci√≥n por posici√≥n con .iloc:\")\n",
    "print(df_ventas.iloc[2:5, 0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìä Operaciones de agrupaci√≥n y agregaci√≥n\n",
    "\n",
    "Las operaciones de agrupaci√≥n permiten realizar an√°lisis por categor√≠as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular total de ventas\n",
    "df_ventas['Total'] = df_ventas['Cantidad'] * df_ventas['Precio']\n",
    "\n",
    "# Agrupar por producto\n",
    "print(\"Ventas por producto:\")\n",
    "ventas_por_producto = df_ventas.groupby('Producto').agg({\n",
    "    'Cantidad': 'sum',\n",
    "    'Total': 'sum'\n",
    "})\n",
    "print(ventas_por_producto)\n",
    "\n",
    "# Agrupar por vendedor\n",
    "print(\"\\nVentas por vendedor:\")\n",
    "ventas_por_vendedor = df_ventas.groupby('Vendedor').agg({\n",
    "    'Cantidad': 'sum',\n",
    "    'Total': 'sum'\n",
    "})\n",
    "print(ventas_por_vendedor)\n",
    "\n",
    "# M√∫ltiples niveles de agrupaci√≥n\n",
    "print(\"\\nVentas por vendedor y producto:\")\n",
    "ventas_detalladas = df_ventas.groupby(['Vendedor', 'Producto']).agg({\n",
    "    'Cantidad': 'sum',\n",
    "    'Total': 'sum'\n",
    "})\n",
    "print(ventas_detalladas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ Ejercicio Pr√°ctico 1: An√°lisis de ventas\n",
    "\n",
    "Utilizando el DataFrame de ventas, realiza las siguientes tareas:\n",
    "\n",
    "1. Calcula el promedio de ventas por d√≠a\n",
    "2. Encuentra el producto m√°s vendido\n",
    "3. Identifica al vendedor con mayor ingreso total\n",
    "4. Crea una nueva columna 'Mes' y agrupa las ventas por mes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tu c√≥digo aqu√≠\n",
    "\n",
    "# 1. Promedio de ventas por d√≠a\n",
    "\n",
    "# 2. Producto m√°s vendido\n",
    "\n",
    "# 3. Vendedor con mayor ingreso\n",
    "\n",
    "# 4. Ventas por mes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. üîç An√°lisis Exploratorio de Datos (EDA)\n",
    "\n",
    "El An√°lisis Exploratorio de Datos (EDA) es un enfoque para analizar conjuntos de datos con el fin de resumir sus caracter√≠sticas principales, a menudo utilizando m√©todos visuales.\n",
    "\n",
    "### üìä Estad√≠sticas descriptivas\n",
    "\n",
    "Las estad√≠sticas descriptivas nos ayudan a entender la distribuci√≥n y caracter√≠sticas de nuestros datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un DataFrame con datos m√°s complejos para EDA\n",
    "np.random.seed(42)\n",
    "n = 1000\n",
    "\n",
    "df_eda = pd.DataFrame({\n",
    "    'edad': np.random.normal(35, 10, n).astype(int),\n",
    "    'ingreso': np.random.lognormal(10, 0.5, n).astype(int),\n",
    "    'gastos': np.random.lognormal(9, 0.4, n).astype(int),\n",
    "    'score_credito': np.random.normal(700, 100, n).clip(300, 850).astype(int),\n",
    "    'genero': np.random.choice(['M', 'F'], n),\n",
    "    'educacion': np.random.choice(['Primaria', 'Secundaria', 'Universidad', 'Posgrado'], n,\n",
    "                                 p=[0.1, 0.3, 0.4, 0.2]),\n",
    "    'region': np.random.choice(['Norte', 'Sur', 'Este', 'Oeste', 'Centro'], n)\n",
    "})\n",
    "\n",
    "# A√±adir algunas variables derivadas\n",
    "df_eda['ahorro'] = df_eda['ingreso'] - df_eda['gastos']\n",
    "df_eda['ratio_gasto'] = df_eda['gastos'] / df_eda['ingreso']\n",
    "\n",
    "# Mostrar las primeras filas\n",
    "print(\"DataFrame para EDA:\")\n",
    "print(df_eda.head())\n",
    "\n",
    "# Estad√≠sticas descriptivas\n",
    "print(\"\\nEstad√≠sticas descriptivas:\")\n",
    "print(df_eda.describe())\n",
    "\n",
    "# Informaci√≥n sobre variables categ√≥ricas\n",
    "print(\"\\nDistribuci√≥n de g√©nero:\")\n",
    "print(df_eda['genero'].value_counts())\n",
    "\n",
    "print(\"\\nDistribuci√≥n de educaci√≥n:\")\n",
    "print(df_eda['educacion'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìà Visualizaci√≥n con Matplotlib y Seaborn\n",
    "\n",
    "La visualizaci√≥n es clave para entender patrones en los datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuraci√≥n para visualizaciones m√°s atractivas\n",
    "plt.style.use('ggplot')\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Crear una figura con m√∫ltiples subplots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Histograma de edad\n",
    "sns.histplot(df_eda['edad'], kde=True, ax=axes[0, 0])\n",
    "axes[0, 0].set_title('Distribuci√≥n de Edad')\n",
    "\n",
    "# Histograma de ingresos\n",
    "sns.histplot(df_eda['ingreso'], kde=True, ax=axes[0, 1])\n",
    "axes[0, 1].set_title('Distribuci√≥n de Ingresos')\n",
    "\n",
    "# Gr√°fico de barras para educaci√≥n\n",
    "sns.countplot(x='educacion', data=df_eda, ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Nivel Educativo')\n",
    "axes[1, 0].set_xticklabels(axes[1, 0].get_xticklabels(), rotation=45)\n",
    "\n",
    "# Gr√°fico de barras para regi√≥n\n",
    "sns.countplot(x='region', data=df_eda, ax=axes[1, 1])\n",
    "axes[1, 1].set_title('Distribuci√≥n por Regi√≥n')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relaciones entre variables\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Scatter plot: Ingreso vs Gastos\n",
    "sns.scatterplot(x='ingreso', y='gastos', hue='genero', data=df_eda, ax=axes[0])\n",
    "axes[0].set_title('Ingreso vs Gastos')\n",
    "\n",
    "# Box plot: Score de cr√©dito por nivel educativo\n",
    "sns.boxplot(x='educacion', y='score_credito', data=df_eda, ax=axes[1])\n",
    "axes[1].set_title('Score de Cr√©dito por Nivel Educativo')\n",
    "axes[1].set_xticklabels(axes[1].get_xticklabels(), rotation=45)\n",
    "\n",
    "# Violin plot: Ahorro por g√©nero\n",
    "sns.violinplot(x='genero', y='ahorro', data=df_eda, ax=axes[2])\n",
    "axes[2].set_title('Distribuci√≥n de Ahorro por G√©nero')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîÑ Correlaciones y patrones\n",
    "\n",
    "Analizar correlaciones nos ayuda a identificar relaciones entre variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular matriz de correlaci√≥n\n",
    "corr_matrix = df_eda.select_dtypes(include=[np.number]).corr()\n",
    "\n",
    "# Visualizar matriz de correlaci√≥n\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1, fmt='.2f')\n",
    "plt.title('Matriz de Correlaci√≥n')\n",
    "plt.show()\n",
    "\n",
    "# Pairplot para visualizar relaciones entre m√∫ltiples variables\n",
    "sns.pairplot(df_eda[['edad', 'ingreso', 'gastos', 'score_credito', 'genero']], hue='genero')\n",
    "plt.suptitle('Relaciones entre Variables Num√©ricas por G√©nero', y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ Ejercicio Pr√°ctico 2: An√°lisis Exploratorio\n",
    "\n",
    "Utilizando el DataFrame `df_eda`, realiza las siguientes tareas:\n",
    "\n",
    "1. Identifica las variables con mayor correlaci√≥n\n",
    "2. Crea un gr√°fico que muestre la relaci√≥n entre educaci√≥n, ingreso y ahorro\n",
    "3. Analiza si hay diferencias significativas en el ratio de gasto por regi√≥n\n",
    "4. Identifica posibles valores at√≠picos en el score de cr√©dito"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tu c√≥digo aqu√≠\n",
    "\n",
    "# 1. Variables con mayor correlaci√≥n\n",
    "\n",
    "# 2. Relaci√≥n entre educaci√≥n, ingreso y ahorro\n",
    "\n",
    "# 3. Ratio de gasto por regi√≥n\n",
    "\n",
    "# 4. Valores at√≠picos en score de cr√©dito"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. üîß Manipulaci√≥n Avanzada de Datos\n",
    "\n",
    "Esta secci√≥n cubre t√©cnicas avanzadas para manipular y transformar datos, incluyendo operaciones con fechas, manejo de datos faltantes, y transformaciones complejas.\n",
    "\n",
    "### üìÖ Trabajando con fechas y horas\n",
    "\n",
    "Las fechas son un tipo de dato crucial en el an√°lisis de datos. Pandas proporciona herramientas poderosas para trabajar con ellas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trabajando con fechas y horas\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "\n",
    "# Crear un rango de fechas\n",
    "fechas = pd.date_range(start='2023-01-01', end='2023-12-31', freq='D')\n",
    "print(f\"Generadas {len(fechas)} fechas desde enero hasta diciembre 2023\")\n",
    "\n",
    "# Crear un DataFrame con datos de series temporales\n",
    "df_tiempo = pd.DataFrame({\n",
    "    'fecha': pd.date_range(start='2023-01-01', periods=365, freq='D'),\n",
    "    'ventas': np.random.normal(1000, 200, 365),\n",
    "    'temperatura': np.random.normal(20, 10, 365)\n",
    "})\n",
    "\n",
    "# Establecer fecha como √≠ndice\n",
    "df_tiempo.set_index('fecha', inplace=True)\n",
    "print(\"\\nDataFrame con √≠ndice de fecha:\")\n",
    "print(df_tiempo.head())\n",
    "\n",
    "# Extraer componentes de fecha\n",
    "df_tiempo['a√±o'] = df_tiempo.index.year\n",
    "df_tiempo['mes'] = df_tiempo.index.month\n",
    "df_tiempo['d√≠a_semana'] = df_tiempo.index.dayofweek\n",
    "df_tiempo['nombre_mes'] = df_tiempo.index.month_name()\n",
    "\n",
    "print(\"\\nComponentes de fecha extra√≠dos:\")\n",
    "print(df_tiempo[['a√±o', 'mes', 'd√≠a_semana', 'nombre_mes']].head())\n",
    "\n",
    "# Resample: agregar datos por periodo\n",
    "ventas_mensuales = df_tiempo['ventas'].resample('M').agg(['sum', 'mean', 'std'])\n",
    "print(\"\\nVentas agregadas por mes:\")\n",
    "print(ventas_mensuales.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üï≥Ô∏è Estrategias avanzadas para datos faltantes\n",
    "\n",
    "El manejo de datos faltantes es crucial para obtener resultados confiables en el an√°lisis de datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un DataFrame con patrones de datos faltantes\n",
    "np.random.seed(42)\n",
    "df_faltantes = pd.DataFrame({\n",
    "    'A': np.random.randn(1000),\n",
    "    'B': np.random.randn(1000),\n",
    "    'C': np.random.randn(1000),\n",
    "    'categoria': np.random.choice(['X', 'Y', 'Z'], 1000)\n",
    "})\n",
    "\n",
    "# Introducir valores faltantes de manera realista\n",
    "missing_mask_A = np.random.random(1000) < 0.1  # 10% faltantes\n",
    "missing_mask_B = np.random.random(1000) < 0.05  # 5% faltantes\n",
    "missing_mask_C = (df_faltantes['A'] > 2) | (np.random.random(1000) < 0.03)  # Faltantes no aleatorios\n",
    "\n",
    "df_faltantes.loc[missing_mask_A, 'A'] = np.nan\n",
    "df_faltantes.loc[missing_mask_B, 'B'] = np.nan\n",
    "df_faltantes.loc[missing_mask_C, 'C'] = np.nan\n",
    "\n",
    "print(\"Patrones de datos faltantes:\")\n",
    "print(df_faltantes.isna().sum())\n",
    "print(f\"\\nTotal de filas: {len(df_faltantes)}\")\n",
    "print(f\"Filas sin valores faltantes: {len(df_faltantes.dropna())}\")\n",
    "\n",
    "# An√°lisis de patrones de datos faltantes\n",
    "import missingno as msno\n",
    "print(\"\\nPatr√≥n de valores faltantes (usando missingno):\")\n",
    "# msno.matrix(df_faltantes)  # Comentado para evitar problemas de visualizaci√≥n\n",
    "\n",
    "# Estrategias de imputaci√≥n\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "\n",
    "# 1. Imputaci√≥n por media/mediana\n",
    "imputer_mean = SimpleImputer(strategy='mean')\n",
    "df_mean = df_faltantes.copy()\n",
    "df_mean[['A', 'B', 'C']] = imputer_mean.fit_transform(df_faltantes[['A', 'B', 'C']])\n",
    "\n",
    "# 2. Imputaci√≥n por KNN\n",
    "imputer_knn = KNNImputer(n_neighbors=5)\n",
    "df_knn = df_faltantes.copy()\n",
    "df_knn[['A', 'B', 'C']] = imputer_knn.fit_transform(df_faltantes[['A', 'B', 'C']])\n",
    "\n",
    "# 3. Imputaci√≥n por grupo\n",
    "df_group = df_faltantes.copy()\n",
    "df_group['A'] = df_group.groupby('categoria')['A'].transform(lambda x: x.fillna(x.mean()))\n",
    "df_group['B'] = df_group.groupby('categoria')['B'].transform(lambda x: x.fillna(x.median()))\n",
    "\n",
    "print(\"\\nComparaci√≥n de estrategias de imputaci√≥n:\")\n",
    "print(f\"Original - Media A: {df_faltantes['A'].mean():.3f}\")\n",
    "print(f\"Imputaci√≥n media - Media A: {df_mean['A'].mean():.3f}\")\n",
    "print(f\"Imputaci√≥n KNN - Media A: {df_knn['A'].mean():.3f}\")\n",
    "print(f\"Imputaci√≥n por grupo - Media A: {df_group['A'].mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîÑ Transformaciones de datos\n",
    "\n",
    "Las transformaciones de datos nos permiten preparar los datos para an√°lisis espec√≠ficos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformaciones comunes en an√°lisis de datos\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
    "\n",
    "# Crear datos para transformaciones\n",
    "df_transform = pd.DataFrame({\n",
    "    'valor': [1, 100, 1000, 10000, 100000],\n",
    "    'score': [0.1, 0.5, 0.8, 0.9, 0.95],\n",
    "    'categoria': ['bajo', 'medio', 'alto', 'muy_alto', 'excelente'],\n",
    "    'precio': [10.5, 25.0, 45.2, 78.9, 150.0]\n",
    "})\n",
    "\n",
    "print(\"Datos originales:\")\n",
    "print(df_transform)\n",
    "\n",
    "# 1. Transformaci√≥n logar√≠tmica\n",
    "df_transform['log_valor'] = np.log1p(df_transform['valor'])  # log(1+x) para evitar log(0)\n",
    "\n",
    "# 2. Normalizaci√≥n (Min-Max scaling)\n",
    "scaler_minmax = MinMaxScaler()\n",
    "df_transform['precio_norm'] = scaler_minmax.fit_transform(df_transform[['precio']])\n",
    "\n",
    "# 3. Estandarizaci√≥n (Z-score)\n",
    "scaler_standard = StandardScaler()\n",
    "df_transform['precio_std'] = scaler_standard.fit_transform(df_transform[['precio']])\n",
    "\n",
    "# 4. Codificaci√≥n de variables categ√≥ricas\n",
    "# One-hot encoding\n",
    "categorias_dummy = pd.get_dummies(df_transform['categoria'], prefix='cat')\n",
    "df_transform = pd.concat([df_transform, categorias_dummy], axis=1)\n",
    "\n",
    "# Label encoding\n",
    "label_encoder = LabelEncoder()\n",
    "df_transform['categoria_encoded'] = label_encoder.fit_transform(df_transform['categoria'])\n",
    "\n",
    "print(\"\\nDatos despu√©s de transformaciones:\")\n",
    "print(df_transform)\n",
    "\n",
    "# 5. Binning (discretizaci√≥n)\n",
    "df_transform['precio_bins'] = pd.cut(df_transform['precio'], \n",
    "                                   bins=3, \n",
    "                                   labels=['Bajo', 'Medio', 'Alto'])\n",
    "\n",
    "print(\"\\nBinning de precios:\")\n",
    "print(df_transform[['precio', 'precio_bins']])\n",
    "\n",
    "# 6. Transformaci√≥n de ranking\n",
    "df_transform['precio_rank'] = df_transform['precio'].rank(method='dense')\n",
    "\n",
    "print(\"\\nRanking de precios:\")\n",
    "print(df_transform[['precio', 'precio_rank']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. üìä Visualizaci√≥n Avanzada\n",
    "\n",
    "La visualizaci√≥n efectiva es clave para comunicar insights de datos. Exploraremos t√©cnicas avanzadas con Plotly y visualizaciones interactivas.\n",
    "\n",
    "### üé® Visualizaciones interactivas con Plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizaciones avanzadas con Plotly\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Crear datos m√°s complejos para visualizaci√≥n\n",
    "np.random.seed(42)\n",
    "n = 500\n",
    "\n",
    "df_viz = pd.DataFrame({\n",
    "    'x': np.random.normal(0, 1, n),\n",
    "    'y': np.random.normal(0, 1, n),\n",
    "    'z': np.random.normal(0, 1, n),\n",
    "    'categoria': np.random.choice(['A', 'B', 'C', 'D'], n),\n",
    "    'tama√±o': np.random.uniform(10, 100, n),\n",
    "    'fecha': pd.date_range('2023-01-01', periods=n, freq='D'),\n",
    "    'valor': np.random.exponential(2, n)\n",
    "})\n",
    "\n",
    "# 1. Scatter plot interactivo con m√∫ltiples dimensiones\n",
    "fig1 = px.scatter(df_viz, \n",
    "                  x='x', \n",
    "                  y='y', \n",
    "                  color='categoria',\n",
    "                  size='tama√±o',\n",
    "                  hover_data=['z', 'valor'],\n",
    "                  title='Scatter Plot Multidimensional Interactivo')\n",
    "fig1.show()\n",
    "\n",
    "# 2. Gr√°fico de series temporales interactivo\n",
    "df_time_agg = df_viz.groupby(['fecha', 'categoria'])['valor'].sum().reset_index()\n",
    "\n",
    "fig2 = px.line(df_time_agg, \n",
    "               x='fecha', \n",
    "               y='valor', \n",
    "               color='categoria',\n",
    "               title='Series Temporales por Categor√≠a')\n",
    "fig2.show()\n",
    "\n",
    "# 3. Histograma con distribuciones superpuestas\n",
    "fig3 = px.histogram(df_viz, \n",
    "                    x='valor', \n",
    "                    color='categoria',\n",
    "                    marginal='box',  # A√±adir boxplot en el margen\n",
    "                    title='Distribuci√≥n de Valores por Categor√≠a')\n",
    "fig3.show()\n",
    "\n",
    "# 4. Gr√°fico 3D\n",
    "fig4 = px.scatter_3d(df_viz, \n",
    "                     x='x', \n",
    "                     y='y', \n",
    "                     z='z',\n",
    "                     color='categoria',\n",
    "                     size='tama√±o',\n",
    "                     title='Visualizaci√≥n 3D')\n",
    "fig4.show()\n",
    "\n",
    "print(\"Visualizaciones interactivas creadas con Plotly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. üåê Extracci√≥n de Datos\n",
    "\n",
    "En el mundo real, los datos provienen de m√∫ltiples fuentes. Aprenderemos a extraer datos de APIs, p√°ginas web y bases de datos.\n",
    "\n",
    "### üîó Consumo de APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracci√≥n de datos de APIs\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# Ejemplo con API p√∫blica (JSONPlaceholder)\n",
    "def obtener_datos_api():\n",
    "    \"\"\"Funci√≥n para obtener datos de una API p√∫blica\"\"\"\n",
    "    try:\n",
    "        # API de ejemplo que no requiere autenticaci√≥n\n",
    "        url = \"https://jsonplaceholder.typicode.com/posts\"\n",
    "        response = requests.get(url)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            datos = response.json()\n",
    "            df_api = pd.DataFrame(datos)\n",
    "            return df_api\n",
    "        else:\n",
    "            print(f\"Error en la API: {response.status_code}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error al conectar con la API: {e}\")\n",
    "        return None\n",
    "\n",
    "# Intentar obtener datos de la API\n",
    "df_posts = obtener_datos_api()\n",
    "\n",
    "if df_posts is not None:\n",
    "    print(\"Datos obtenidos de la API:\")\n",
    "    print(df_posts.head())\n",
    "    print(f\"\\nTotal de posts: {len(df_posts)}\")\n",
    "    print(f\"Columnas: {df_posts.columns.tolist()}\")\n",
    "else:\n",
    "    print(\"No se pudieron obtener datos de la API\")\n",
    "\n",
    "# Ejemplo de an√°lisis b√°sico de los datos de la API\n",
    "if df_posts is not None:\n",
    "    # An√°lisis de longitud de t√≠tulos\n",
    "    df_posts['titulo_longitud'] = df_posts['title'].str.len()\n",
    "    df_posts['body_longitud'] = df_posts['body'].str.len()\n",
    "    \n",
    "    print(\"\\nEstad√≠sticas de longitud:\")\n",
    "    print(df_posts[['titulo_longitud', 'body_longitud']].describe())\n",
    "    \n",
    "    # Usuarios m√°s activos\n",
    "    usuarios_activos = df_posts['userId'].value_counts().head()\n",
    "    print(\"\\nUsuarios m√°s activos:\")\n",
    "    print(usuarios_activos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üï∏Ô∏è Web Scraping con BeautifulSoup\n",
    "\n",
    "El web scraping nos permite extraer datos de p√°ginas web. Es importante hacerlo de manera √©tica y respetando los t√©rminos de servicio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Web Scraping con BeautifulSoup\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "\n",
    "# Funci√≥n para scraping √©tico\n",
    "def scraping_ejemplo():\n",
    "    \"\"\"\n",
    "    Ejemplo de web scraping b√°sico\n",
    "    Nota: Siempre revisar robots.txt y t√©rminos de servicio\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # URL de ejemplo (sitio que permite scraping)\n",
    "        url = \"https://httpbin.org/html\"\n",
    "        \n",
    "        # Hacer la petici√≥n con headers apropiados\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "        }\n",
    "        \n",
    "        response = requests.get(url, headers=headers)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            # Extraer informaci√≥n\n",
    "            titulo = soup.find('title')\n",
    "            encabezados = soup.find_all(['h1', 'h2', 'h3'])\n",
    "            parrafos = soup.find_all('p')\n",
    "            \n",
    "            print(\"Scraping exitoso:\")\n",
    "            print(f\"T√≠tulo: {titulo.text if titulo else 'No encontrado'}\")\n",
    "            print(f\"N√∫mero de encabezados: {len(encabezados)}\")\n",
    "            print(f\"N√∫mero de p√°rrafos: {len(parrafos)}\")\n",
    "            \n",
    "            return True\n",
    "        else:\n",
    "            print(f\"Error al acceder a la p√°gina: {response.status_code}\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error en scraping: {e}\")\n",
    "        return False\n",
    "\n",
    "# Ejecutar ejemplo de scraping\n",
    "resultado_scraping = scraping_ejemplo()\n",
    "\n",
    "# Ejemplo de creaci√≥n de datos simulados para an√°lisis\n",
    "# (En lugar de scraping real por limitaciones del entorno)\n",
    "def crear_datos_simulados_web():\n",
    "    \"\"\"Crear datos que simular√≠an venir de web scraping\"\"\"\n",
    "    productos = ['Laptop', 'Mouse', 'Teclado', 'Monitor', 'Auriculares']\n",
    "    tiendas = ['TiendaA', 'TiendaB', 'TiendaC']\n",
    "    \n",
    "    datos_simulados = []\n",
    "    \n",
    "    for _ in range(100):\n",
    "        producto = np.random.choice(productos)\n",
    "        tienda = np.random.choice(tiendas)\n",
    "        precio = np.random.uniform(50, 2000)\n",
    "        rating = np.random.uniform(1, 5)\n",
    "        stock = np.random.randint(0, 100)\n",
    "        \n",
    "        datos_simulados.append({\n",
    "            'producto': producto,\n",
    "            'tienda': tienda,\n",
    "            'precio': round(precio, 2),\n",
    "            'rating': round(rating, 1),\n",
    "            'stock': stock,\n",
    "            'fecha_scraping': pd.Timestamp.now()\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(datos_simulados)\n",
    "\n",
    "# Crear datos simulados\n",
    "df_scraping = crear_datos_simulados_web()\n",
    "print(\"\\nDatos simulados de web scraping:\")\n",
    "print(df_scraping.head())\n",
    "print(f\"\\nTotal de productos scrapeados: {len(df_scraping)}\")\n",
    "\n",
    "# An√°lisis de los datos \"scrapeados\"\n",
    "print(\"\\nAn√°lisis de precios por tienda:\")\n",
    "precio_por_tienda = df_scraping.groupby('tienda')['precio'].agg(['mean', 'min', 'max'])\n",
    "print(precio_por_tienda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. ü¶† Proyecto Integrador: An√°lisis de Datos de COVID-19\n",
    "\n",
    "Aplicaremos todas las t√©cnicas aprendidas en un proyecto real: an√°lisis de datos de COVID-19.\n",
    "\n",
    "### üìã Objetivos del proyecto:\n",
    "1. Obtener datos de COVID-19 de fuentes confiables\n",
    "2. Limpiar y preparar los datos\n",
    "3. Realizar an√°lisis exploratorio\n",
    "4. Crear visualizaciones informativas\n",
    "5. Identificar patrones y tendencias\n",
    "6. Generar insights actionables\n",
    "\n",
    "### üìä Carga y preparaci√≥n de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proyecto COVID-19: An√°lisis de datos completo\n",
    "\n",
    "# Como no podemos acceder a APIs externas en este entorno,\n",
    "# simularemos datos realistas de COVID-19\n",
    "def crear_datos_covid_simulados():\n",
    "    \"\"\"Crear datos realistas que simulan datos de COVID-19\"\"\"\n",
    "    \n",
    "    # Pa√≠ses para el an√°lisis\n",
    "    paises = ['Espa√±a', 'Francia', 'Italia', 'Alemania', 'Reino Unido', \n",
    "              'Estados Unidos', 'Brasil', 'India', 'China', 'Jap√≥n']\n",
    "    \n",
    "    # Generar fechas para un a√±o completo\n",
    "    fechas = pd.date_range(start='2023-01-01', end='2023-12-31', freq='D')\n",
    "    \n",
    "    datos_covid = []\n",
    "    \n",
    "    for pais in paises:\n",
    "        # Simular tendencias diferentes por pa√≠s\n",
    "        base_casos = np.random.randint(1000, 10000)\n",
    "        tendencia = np.random.uniform(-0.001, 0.001)\n",
    "        \n",
    "        for i, fecha in enumerate(fechas):\n",
    "            # Simular variaci√≥n estacional\n",
    "            variacion_estacional = np.sin(2 * np.pi * i / 365) * 0.3\n",
    "            \n",
    "            # Casos con tendencia y ruido\n",
    "            casos = int(base_casos * (1 + tendencia * i + variacion_estacional) * \n",
    "                       (1 + np.random.normal(0, 0.2)))\n",
    "            casos = max(0, casos)  # No casos negativos\n",
    "            \n",
    "            # Muertes (aproximadamente 1-3% de casos)\n",
    "            muertes = int(casos * np.random.uniform(0.01, 0.03))\n",
    "            \n",
    "            # Recuperados (aproximadamente 95% de casos con retraso)\n",
    "            recuperados = int(casos * 0.95 * (1 + np.random.normal(0, 0.1)))\n",
    "            \n",
    "            # Poblaci√≥n aproximada por pa√≠s (en millones)\n",
    "            poblaciones = {\n",
    "                'Espa√±a': 47, 'Francia': 68, 'Italia': 60, 'Alemania': 83,\n",
    "                'Reino Unido': 67, 'Estados Unidos': 331, 'Brasil': 215,\n",
    "                'India': 1380, 'China': 1441, 'Jap√≥n': 125\n",
    "            }\n",
    "            \n",
    "            datos_covid.append({\n",
    "                'fecha': fecha,\n",
    "                'pais': pais,\n",
    "                'casos_nuevos': casos,\n",
    "                'muertes_nuevas': muertes,\n",
    "                'recuperados_nuevos': recuperados,\n",
    "                'poblacion_millones': poblaciones[pais]\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(datos_covid)\n",
    "\n",
    "# Crear el dataset de COVID-19\n",
    "df_covid = crear_datos_covid_simulados()\n",
    "\n",
    "print(\"Dataset de COVID-19 creado:\")\n",
    "print(f\"Per√≠odo: {df_covid['fecha'].min()} a {df_covid['fecha'].max()}\")\n",
    "print(f\"Pa√≠ses: {len(df_covid['pais'].unique())}\")\n",
    "print(f\"Registros totales: {len(df_covid)}\")\n",
    "print(\"\\nPrimeras filas:\")\n",
    "print(df_covid.head())\n",
    "\n",
    "# Calcular m√©tricas acumuladas\n",
    "df_covid['casos_acumulados'] = df_covid.groupby('pais')['casos_nuevos'].cumsum()\n",
    "df_covid['muertes_acumuladas'] = df_covid.groupby('pais')['muertes_nuevas'].cumsum()\n",
    "df_covid['recuperados_acumulados'] = df_covid.groupby('pais')['recuperados_nuevos'].cumsum()\n",
    "\n",
    "# Calcular tasas por mill√≥n de habitantes\n",
    "df_covid['casos_por_millon'] = (df_covid['casos_acumulados'] / df_covid['poblacion_millones'])\n",
    "df_covid['muertes_por_millon'] = (df_covid['muertes_acumuladas'] / df_covid['poblacion_millones'])\n",
    "\n",
    "print(\"\\nM√©tricas calculadas:\")\n",
    "print(df_covid[['pais', 'fecha', 'casos_acumulados', 'casos_por_millon']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lisis exploratorio de datos COVID-19\n",
    "\n",
    "# 1. Resumen estad√≠stico por pa√≠s\n",
    "print(\"Resumen estad√≠stico por pa√≠s (al final del per√≠odo):\")\n",
    "resumen_final = df_covid.groupby('pais').tail(1)[['pais', 'casos_acumulados', \n",
    "                                                  'muertes_acumuladas', \n",
    "                                                  'casos_por_millon']].set_index('pais')\n",
    "resumen_final = resumen_final.sort_values('casos_acumulados', ascending=False)\n",
    "print(resumen_final)\n",
    "\n",
    "# 2. Pa√≠ses m√°s afectados\n",
    "print(\"\\nTop 5 pa√≠ses por casos totales:\")\n",
    "top_casos = resumen_final.sort_values('casos_acumulados', ascending=False).head()\n",
    "print(top_casos[['casos_acumulados', 'muertes_acumuladas']])\n",
    "\n",
    "print(\"\\nTop 5 pa√≠ses por casos por mill√≥n de habitantes:\")\n",
    "top_per_capita = resumen_final.sort_values('casos_por_millon', ascending=False).head()\n",
    "print(top_per_capita[['casos_por_millon']])\n",
    "\n",
    "# 3. Visualizaciones del proyecto\n",
    "\n",
    "# Configurar estilo\n",
    "plt.style.use('ggplot')\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Gr√°fico 1: Evoluci√≥n temporal de casos por pa√≠s (pa√≠ses principales)\n",
    "paises_principales = ['Estados Unidos', 'Brasil', 'India', 'Espa√±a', 'Francia']\n",
    "for pais in paises_principales:\n",
    "    data_pais = df_covid[df_covid['pais'] == pais]\n",
    "    axes[0, 0].plot(data_pais['fecha'], data_pais['casos_acumulados'], \n",
    "                    label=pais, linewidth=2)\n",
    "\n",
    "axes[0, 0].set_title('Evoluci√≥n de Casos Acumulados', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Fecha')\n",
    "axes[0, 0].set_ylabel('Casos Acumulados')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Gr√°fico 2: Casos por mill√≥n al final del per√≠odo\n",
    "axes[0, 1].bar(range(len(top_per_capita)), top_per_capita['casos_por_millon'].values)\n",
    "axes[0, 1].set_title('Casos por Mill√≥n de Habitantes', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Pa√≠s')\n",
    "axes[0, 1].set_ylabel('Casos por Mill√≥n')\n",
    "axes[0, 1].set_xticks(range(len(top_per_capita)))\n",
    "axes[0, 1].set_xticklabels(top_per_capita.index, rotation=45)\n",
    "\n",
    "# Gr√°fico 3: Comparaci√≥n casos nuevos vs muertes nuevas\n",
    "# Promedio m√≥vil de 7 d√≠as para suavizar\n",
    "df_covid['casos_ma7'] = df_covid.groupby('pais')['casos_nuevos'].rolling(7).mean().reset_index(0, drop=True)\n",
    "df_covid['muertes_ma7'] = df_covid.groupby('pais')['muertes_nuevas'].rolling(7).mean().reset_index(0, drop=True)\n",
    "\n",
    "# Seleccionar Espa√±a para el ejemplo\n",
    "data_espana = df_covid[df_covid['pais'] == 'Espa√±a']\n",
    "axes[1, 0].plot(data_espana['fecha'], data_espana['casos_ma7'], \n",
    "                label='Casos (MA7)', color='blue', linewidth=2)\n",
    "ax2 = axes[1, 0].twinx()\n",
    "ax2.plot(data_espana['fecha'], data_espana['muertes_ma7'], \n",
    "         label='Muertes (MA7)', color='red', linewidth=2)\n",
    "\n",
    "axes[1, 0].set_title('Espa√±a: Casos vs Muertes (Media M√≥vil 7 d√≠as)', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Fecha')\n",
    "axes[1, 0].set_ylabel('Casos Nuevos', color='blue')\n",
    "ax2.set_ylabel('Muertes Nuevas', color='red')\n",
    "\n",
    "# Gr√°fico 4: Distribuci√≥n de casos por mes\n",
    "df_covid['mes'] = df_covid['fecha'].dt.month\n",
    "casos_por_mes = df_covid.groupby('mes')['casos_nuevos'].sum()\n",
    "axes[1, 1].bar(casos_por_mes.index, casos_por_mes.values)\n",
    "axes[1, 1].set_title('Distribuci√≥n de Casos por Mes', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Mes')\n",
    "axes[1, 1].set_ylabel('Total de Casos Nuevos')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 4. An√°lisis de correlaciones\n",
    "print(\"\\n=== AN√ÅLISIS DE CORRELACIONES ===\")\n",
    "correlaciones = df_covid[['casos_nuevos', 'muertes_nuevas', 'recuperados_nuevos', \n",
    "                         'poblacion_millones']].corr()\n",
    "print(\"Matriz de correlaci√≥n:\")\n",
    "print(correlaciones)\n",
    "\n",
    "# 5. Identificaci√≥n de patrones estacionales\n",
    "print(\"\\n=== AN√ÅLISIS ESTACIONAL ===\")\n",
    "casos_estacionales = df_covid.groupby(['mes'])['casos_nuevos'].agg(['mean', 'std'])\n",
    "print(\"Patrones estacionales (promedio de casos por mes):\")\n",
    "print(casos_estacionales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìã Insights y Conclusiones del Proyecto\n",
    "\n",
    "Del an√°lisis de datos COVID-19 podemos extraer varios insights importantes:\n",
    "\n",
    "#### üîç Hallazgos principales:\n",
    "1. **Variabilidad entre pa√≠ses**: Los patrones de contagio var√≠an significativamente\n",
    "2. **Tendencias estacionales**: Se observan patrones estacionales en algunos pa√≠ses\n",
    "3. **Correlaci√≥n casos-muertes**: Existe una correlaci√≥n positiva pero con variaciones temporales\n",
    "4. **Impacto poblacional**: Los casos por mill√≥n de habitantes ofrecen una perspectiva m√°s equilibrada\n",
    "\n",
    "#### üí° Recomendaciones para an√°lisis futuros:\n",
    "- Incorporar datos de vacunaci√≥n\n",
    "- Analizar impacto de medidas de pol√≠tica p√∫blica\n",
    "- Estudiar correlaciones con factores socioecon√≥micos\n",
    "- Implementar modelos predictivos\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Ejercicios Finales del M√≥dulo\n",
    "\n",
    "### Ejercicio 1: An√°lisis personalizado\n",
    "Elige un subconjunto de pa√≠ses y realiza un an√°lisis comparativo detallado incluyendo:\n",
    "- Evoluci√≥n temporal\n",
    "- Tasas de mortalidad\n",
    "- Patrones estacionales\n",
    "- Visualizaciones personalizadas\n",
    "\n",
    "### Ejercicio 2: Dashboard interactivo\n",
    "Crea un dashboard interactivo usando Plotly que permita:\n",
    "- Seleccionar pa√≠ses\n",
    "- Filtrar por fechas\n",
    "- Comparar m√©tricas\n",
    "- Mostrar tendencias\n",
    "\n",
    "### Ejercicio 3: An√°lisis predictivo b√°sico\n",
    "Implementa un modelo simple para predecir:\n",
    "- Tendencias futuras de casos\n",
    "- Patrones estacionales\n",
    "- Comparaci√≥n de diferentes enfoques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéâ ¬°Felicitaciones! Has completado el M√≥dulo 5\n",
    "\n",
    "### üìö Resumen de lo aprendido:\n",
    "\n",
    "1. **‚úÖ Fundamentos del an√°lisis de datos**\n",
    "   - Ecosistema de Python para datos\n",
    "   - Flujo de trabajo t√≠pico\n",
    "\n",
    "2. **‚úÖ Manipulaci√≥n con Pandas**\n",
    "   - Series y DataFrames\n",
    "   - Limpieza y transformaci√≥n\n",
    "   - Operaciones de agrupaci√≥n\n",
    "\n",
    "3. **‚úÖ An√°lisis exploratorio (EDA)**\n",
    "   - Estad√≠sticas descriptivas\n",
    "   - Visualizaciones efectivas\n",
    "   - Identificaci√≥n de patrones\n",
    "\n",
    "4. **‚úÖ T√©cnicas avanzadas**\n",
    "   - Manejo de fechas y datos faltantes\n",
    "   - Transformaciones de datos\n",
    "   - Imputaci√≥n inteligente\n",
    "\n",
    "5. **‚úÖ Visualizaci√≥n profesional**\n",
    "   - Gr√°ficos interactivos con Plotly\n",
    "   - Dashboards informativos\n",
    "   - Comunicaci√≥n visual de insights\n",
    "\n",
    "6. **‚úÖ Extracci√≥n de datos**\n",
    "   - Consumo de APIs\n",
    "   - Web scraping √©tico\n",
    "   - Integraci√≥n de m√∫ltiples fuentes\n",
    "\n",
    "7. **‚úÖ Proyecto integrador**\n",
    "   - An√°lisis completo de COVID-19\n",
    "   - Aplicaci√≥n pr√°ctica de todas las t√©cnicas\n",
    "   - Generaci√≥n de insights actionables\n",
    "\n",
    "### üöÄ Pr√≥ximo m√≥dulo: Machine Learning\n",
    "\n",
    "En el **M√≥dulo 6** exploraremos:\n",
    "- Algoritmos de aprendizaje autom√°tico\n",
    "- Scikit-learn y modelos predictivos\n",
    "- Evaluaci√≥n y validaci√≥n de modelos\n",
    "- Proyectos de ML en producci√≥n\n",
    "\n",
    "### üìñ Recursos adicionales recomendados:\n",
    "\n",
    "- **Libros**: \"Python for Data Analysis\" by Wes McKinney\n",
    "- **Documentaci√≥n**: Pandas, Matplotlib, Plotly\n",
    "- **Pr√°ctica**: Kaggle datasets y competiciones\n",
    "- **Comunidad**: Stack Overflow, Reddit r/MachineLearning\n",
    "\n",
    "¬°Est√°s listo para el siguiente nivel! üéØ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
