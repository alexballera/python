{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📊 Análisis de Datos con Python - Módulo 5\n",
    "\n",
    "## Bienvenido al Módulo de Análisis de Datos\n",
    "\n",
    "### 📚 Contenido del Módulo 5:\n",
    "1. **Introducción al análisis de datos**\n",
    "2. **Manipulación de datos con Pandas**\n",
    "3. **Análisis exploratorio de datos (EDA)**\n",
    "4. **Manipulación avanzada de datos**\n",
    "5. **Visualización avanzada**\n",
    "6. **Extracción de datos**\n",
    "7. **Proyecto: Análisis de datos de COVID-19**\n",
    "\n",
    "### 🎯 Objetivos de Aprendizaje:\n",
    "- Dominar la manipulación de datos con Pandas\n",
    "- Realizar análisis exploratorio efectivo\n",
    "- Crear visualizaciones impactantes\n",
    "- Extraer datos de diversas fuentes\n",
    "- Aplicar técnicas de análisis a datos reales\n",
    "- Construir dashboards interactivos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. 📈 Introducción al Análisis de Datos\n",
    "\n",
    "El análisis de datos es el proceso de inspeccionar, limpiar, transformar y modelar datos con el objetivo de descubrir información útil, informar conclusiones y apoyar la toma de decisiones.\n",
    "\n",
    "### 🌟 ¿Por qué Python para análisis de datos?\n",
    "\n",
    "Python se ha convertido en el lenguaje preferido para análisis de datos por varias razones:\n",
    "\n",
    "- **Ecosistema rico**: Pandas, NumPy, Matplotlib, Scikit-learn\n",
    "- **Sintaxis clara y legible**: Facilita el desarrollo y mantenimiento\n",
    "- **Comunidad activa**: Abundantes recursos y bibliotecas\n",
    "- **Versatilidad**: Desde análisis básico hasta machine learning avanzado\n",
    "- **Integración**: Funciona bien con otras herramientas y sistemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.3' requires the ipykernel package.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
      "\u001b[1;31mOr install 'ipykernel' using the command: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Importar las bibliotecas principales para análisis de datos\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configuración para visualizaciones más atractivas\n",
    "plt.style.use('ggplot')\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Mostrar versiones\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Matplotlib version: {plt.__version__}\")\n",
    "print(f\"Seaborn version: {sns.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🔄 Flujo de trabajo típico en análisis de datos\n",
    "\n",
    "1. **Obtención de datos**: Recolectar datos de diversas fuentes\n",
    "2. **Limpieza de datos**: Manejar valores faltantes, duplicados, errores\n",
    "3. **Exploración**: Análisis estadístico y visualización inicial\n",
    "4. **Transformación**: Crear nuevas variables, normalizar, agregar\n",
    "5. **Análisis**: Aplicar técnicas estadísticas y modelos\n",
    "6. **Visualización**: Crear gráficos y dashboards informativos\n",
    "7. **Comunicación**: Presentar hallazgos y conclusiones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. 🐼 Manipulación de datos con Pandas\n",
    "\n",
    "Pandas es la biblioteca más importante para manipulación y análisis de datos en Python. Proporciona estructuras de datos flexibles y herramientas para trabajar con datos estructurados.\n",
    "\n",
    "### 📋 Estructuras de datos principales:\n",
    "\n",
    "1. **Series**: Array unidimensional etiquetado\n",
    "2. **DataFrame**: Tabla bidimensional con columnas potencialmente de diferentes tipos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear una Series\n",
    "s = pd.Series([1, 3, 5, 7, 9], index=['a', 'b', 'c', 'd', 'e'])\n",
    "print(\"Series de Pandas:\")\n",
    "print(s)\n",
    "print(\"\\nTipo de datos:\", type(s))\n",
    "\n",
    "# Crear un DataFrame\n",
    "data = {\n",
    "    'Nombre': ['Ana', 'Carlos', 'María', 'Pedro', 'Laura'],\n",
    "    'Edad': [28, 34, 29, 42, 31],\n",
    "    'Ciudad': ['Madrid', 'Barcelona', 'Sevilla', 'Valencia', 'Bilbao'],\n",
    "    'Puntuación': [85, 92, 78, 96, 89]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(\"\\nDataFrame de Pandas:\")\n",
    "print(df)\n",
    "\n",
    "# Información básica del DataFrame\n",
    "print(\"\\nInformación del DataFrame:\")\n",
    "print(df.info())\n",
    "\n",
    "# Estadísticas descriptivas\n",
    "print(\"\\nEstadísticas descriptivas:\")\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📥 Carga de datos desde diferentes fuentes\n",
    "\n",
    "Pandas puede leer datos de múltiples formatos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de carga de datos (comentado para evitar errores)\n",
    "\n",
    "# CSV\n",
    "# df_csv = pd.read_csv('datos/ejemplo.csv')\n",
    "\n",
    "# Excel\n",
    "# df_excel = pd.read_excel('datos/ejemplo.xlsx', sheet_name='Hoja1')\n",
    "\n",
    "# JSON\n",
    "# df_json = pd.read_json('datos/ejemplo.json')\n",
    "\n",
    "# SQL (requiere SQLAlchemy)\n",
    "# from sqlalchemy import create_engine\n",
    "# engine = create_engine('sqlite:///ejemplo.db')\n",
    "# df_sql = pd.read_sql('SELECT * FROM tabla', engine)\n",
    "\n",
    "# Crear un DataFrame de ejemplo para continuar\n",
    "df_ventas = pd.DataFrame({\n",
    "    'Fecha': pd.date_range(start='2023-01-01', periods=10),\n",
    "    'Producto': ['A', 'B', 'A', 'C', 'B', 'A', 'C', 'B', 'A', 'C'],\n",
    "    'Cantidad': [10, 5, 15, 8, 12, 20, 7, 9, 14, 11],\n",
    "    'Precio': [100, 200, 100, 150, 200, 100, 150, 200, 100, 150],\n",
    "    'Vendedor': ['Juan', 'María', 'Juan', 'Pedro', 'María', 'Juan', 'Pedro', 'María', 'Juan', 'Pedro']\n",
    "})\n",
    "\n",
    "print(\"DataFrame de ventas:\")\n",
    "print(df_ventas.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🧹 Limpieza y transformación de datos\n",
    "\n",
    "La limpieza de datos es una parte crucial del análisis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un DataFrame con problemas comunes\n",
    "df_sucio = pd.DataFrame({\n",
    "    'A': [1, 2, np.nan, 4, 5],\n",
    "    'B': [5, 6, 7, np.nan, 9],\n",
    "    'C': ['a', 'b', 'c', 'd', np.nan]\n",
    "})\n",
    "\n",
    "print(\"DataFrame con valores faltantes:\")\n",
    "print(df_sucio)\n",
    "\n",
    "# Detectar valores faltantes\n",
    "print(\"\\nValores faltantes por columna:\")\n",
    "print(df_sucio.isna().sum())\n",
    "\n",
    "# Rellenar valores faltantes\n",
    "df_limpio1 = df_sucio.fillna({'A': 0, 'B': 0, 'C': 'desconocido'})\n",
    "print(\"\\nDataFrame con valores faltantes rellenados:\")\n",
    "print(df_limpio1)\n",
    "\n",
    "# Eliminar filas con valores faltantes\n",
    "df_limpio2 = df_sucio.dropna()\n",
    "print(\"\\nDataFrame con filas con valores faltantes eliminadas:\")\n",
    "print(df_limpio2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🔍 Filtrado y selección de datos\n",
    "\n",
    "Pandas ofrece múltiples formas de seleccionar y filtrar datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Volvemos al DataFrame de ventas\n",
    "print(\"DataFrame de ventas:\")\n",
    "print(df_ventas)\n",
    "\n",
    "# Selección de columnas\n",
    "print(\"\\nSelección de columnas:\")\n",
    "print(df_ventas[['Producto', 'Cantidad']])\n",
    "\n",
    "# Filtrado con condiciones\n",
    "print(\"\\nVentas del producto A:\")\n",
    "print(df_ventas[df_ventas['Producto'] == 'A'])\n",
    "\n",
    "# Filtrado con múltiples condiciones\n",
    "print(\"\\nVentas del producto A con cantidad > 10:\")\n",
    "print(df_ventas[(df_ventas['Producto'] == 'A') & (df_ventas['Cantidad'] > 10)])\n",
    "\n",
    "# Selección por índice con .loc\n",
    "print(\"\\nSelección por índice con .loc:\")\n",
    "print(df_ventas.loc[2:4, ['Fecha', 'Producto', 'Cantidad']])\n",
    "\n",
    "# Selección por posición con .iloc\n",
    "print(\"\\nSelección por posición con .iloc:\")\n",
    "print(df_ventas.iloc[2:5, 0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📊 Operaciones de agrupación y agregación\n",
    "\n",
    "Las operaciones de agrupación permiten realizar análisis por categorías:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular total de ventas\n",
    "df_ventas['Total'] = df_ventas['Cantidad'] * df_ventas['Precio']\n",
    "\n",
    "# Agrupar por producto\n",
    "print(\"Ventas por producto:\")\n",
    "ventas_por_producto = df_ventas.groupby('Producto').agg({\n",
    "    'Cantidad': 'sum',\n",
    "    'Total': 'sum'\n",
    "})\n",
    "print(ventas_por_producto)\n",
    "\n",
    "# Agrupar por vendedor\n",
    "print(\"\\nVentas por vendedor:\")\n",
    "ventas_por_vendedor = df_ventas.groupby('Vendedor').agg({\n",
    "    'Cantidad': 'sum',\n",
    "    'Total': 'sum'\n",
    "})\n",
    "print(ventas_por_vendedor)\n",
    "\n",
    "# Múltiples niveles de agrupación\n",
    "print(\"\\nVentas por vendedor y producto:\")\n",
    "ventas_detalladas = df_ventas.groupby(['Vendedor', 'Producto']).agg({\n",
    "    'Cantidad': 'sum',\n",
    "    'Total': 'sum'\n",
    "})\n",
    "print(ventas_detalladas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🎯 Ejercicio Práctico 1: Análisis de ventas\n",
    "\n",
    "Utilizando el DataFrame de ventas, realiza las siguientes tareas:\n",
    "\n",
    "1. Calcula el promedio de ventas por día\n",
    "2. Encuentra el producto más vendido\n",
    "3. Identifica al vendedor con mayor ingreso total\n",
    "4. Crea una nueva columna 'Mes' y agrupa las ventas por mes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tu código aquí\n",
    "\n",
    "# 1. Promedio de ventas por día\n",
    "\n",
    "# 2. Producto más vendido\n",
    "\n",
    "# 3. Vendedor con mayor ingreso\n",
    "\n",
    "# 4. Ventas por mes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. 🔍 Análisis Exploratorio de Datos (EDA)\n",
    "\n",
    "El Análisis Exploratorio de Datos (EDA) es un enfoque para analizar conjuntos de datos con el fin de resumir sus características principales, a menudo utilizando métodos visuales.\n",
    "\n",
    "### 📊 Estadísticas descriptivas\n",
    "\n",
    "Las estadísticas descriptivas nos ayudan a entender la distribución y características de nuestros datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un DataFrame con datos más complejos para EDA\n",
    "np.random.seed(42)\n",
    "n = 1000\n",
    "\n",
    "df_eda = pd.DataFrame({\n",
    "    'edad': np.random.normal(35, 10, n).astype(int),\n",
    "    'ingreso': np.random.lognormal(10, 0.5, n).astype(int),\n",
    "    'gastos': np.random.lognormal(9, 0.4, n).astype(int),\n",
    "    'score_credito': np.random.normal(700, 100, n).clip(300, 850).astype(int),\n",
    "    'genero': np.random.choice(['M', 'F'], n),\n",
    "    'educacion': np.random.choice(['Primaria', 'Secundaria', 'Universidad', 'Posgrado'], n,\n",
    "                                 p=[0.1, 0.3, 0.4, 0.2]),\n",
    "    'region': np.random.choice(['Norte', 'Sur', 'Este', 'Oeste', 'Centro'], n)\n",
    "})\n",
    "\n",
    "# Añadir algunas variables derivadas\n",
    "df_eda['ahorro'] = df_eda['ingreso'] - df_eda['gastos']\n",
    "df_eda['ratio_gasto'] = df_eda['gastos'] / df_eda['ingreso']\n",
    "\n",
    "# Mostrar las primeras filas\n",
    "print(\"DataFrame para EDA:\")\n",
    "print(df_eda.head())\n",
    "\n",
    "# Estadísticas descriptivas\n",
    "print(\"\\nEstadísticas descriptivas:\")\n",
    "print(df_eda.describe())\n",
    "\n",
    "# Información sobre variables categóricas\n",
    "print(\"\\nDistribución de género:\")\n",
    "print(df_eda['genero'].value_counts())\n",
    "\n",
    "print(\"\\nDistribución de educación:\")\n",
    "print(df_eda['educacion'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📈 Visualización con Matplotlib y Seaborn\n",
    "\n",
    "La visualización es clave para entender patrones en los datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración para visualizaciones más atractivas\n",
    "plt.style.use('ggplot')\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Crear una figura con múltiples subplots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Histograma de edad\n",
    "sns.histplot(df_eda['edad'], kde=True, ax=axes[0, 0])\n",
    "axes[0, 0].set_title('Distribución de Edad')\n",
    "\n",
    "# Histograma de ingresos\n",
    "sns.histplot(df_eda['ingreso'], kde=True, ax=axes[0, 1])\n",
    "axes[0, 1].set_title('Distribución de Ingresos')\n",
    "\n",
    "# Gráfico de barras para educación\n",
    "sns.countplot(x='educacion', data=df_eda, ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Nivel Educativo')\n",
    "axes[1, 0].set_xticklabels(axes[1, 0].get_xticklabels(), rotation=45)\n",
    "\n",
    "# Gráfico de barras para región\n",
    "sns.countplot(x='region', data=df_eda, ax=axes[1, 1])\n",
    "axes[1, 1].set_title('Distribución por Región')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relaciones entre variables\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Scatter plot: Ingreso vs Gastos\n",
    "sns.scatterplot(x='ingreso', y='gastos', hue='genero', data=df_eda, ax=axes[0])\n",
    "axes[0].set_title('Ingreso vs Gastos')\n",
    "\n",
    "# Box plot: Score de crédito por nivel educativo\n",
    "sns.boxplot(x='educacion', y='score_credito', data=df_eda, ax=axes[1])\n",
    "axes[1].set_title('Score de Crédito por Nivel Educativo')\n",
    "axes[1].set_xticklabels(axes[1].get_xticklabels(), rotation=45)\n",
    "\n",
    "# Violin plot: Ahorro por género\n",
    "sns.violinplot(x='genero', y='ahorro', data=df_eda, ax=axes[2])\n",
    "axes[2].set_title('Distribución de Ahorro por Género')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🔄 Correlaciones y patrones\n",
    "\n",
    "Analizar correlaciones nos ayuda a identificar relaciones entre variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular matriz de correlación\n",
    "corr_matrix = df_eda.select_dtypes(include=[np.number]).corr()\n",
    "\n",
    "# Visualizar matriz de correlación\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1, fmt='.2f')\n",
    "plt.title('Matriz de Correlación')\n",
    "plt.show()\n",
    "\n",
    "# Pairplot para visualizar relaciones entre múltiples variables\n",
    "sns.pairplot(df_eda[['edad', 'ingreso', 'gastos', 'score_credito', 'genero']], hue='genero')\n",
    "plt.suptitle('Relaciones entre Variables Numéricas por Género', y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🎯 Ejercicio Práctico 2: Análisis Exploratorio\n",
    "\n",
    "Utilizando el DataFrame `df_eda`, realiza las siguientes tareas:\n",
    "\n",
    "1. Identifica las variables con mayor correlación\n",
    "2. Crea un gráfico que muestre la relación entre educación, ingreso y ahorro\n",
    "3. Analiza si hay diferencias significativas en el ratio de gasto por región\n",
    "4. Identifica posibles valores atípicos en el score de crédito"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tu código aquí\n",
    "\n",
    "# 1. Variables con mayor correlación\n",
    "\n",
    "# 2. Relación entre educación, ingreso y ahorro\n",
    "\n",
    "# 3. Ratio de gasto por región\n",
    "\n",
    "# 4. Valores atípicos en score de crédito"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. 🔧 Manipulación Avanzada de Datos\n",
    "\n",
    "Esta sección cubre técnicas avanzadas para manipular y transformar datos, incluyendo operaciones con fechas, manejo de datos faltantes, y transformaciones complejas.\n",
    "\n",
    "### 📅 Trabajando con fechas y horas\n",
    "\n",
    "Las fechas son un tipo de dato crucial en el análisis de datos. Pandas proporciona herramientas poderosas para trabajar con ellas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trabajando con fechas y horas\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "\n",
    "# Crear un rango de fechas\n",
    "fechas = pd.date_range(start='2023-01-01', end='2023-12-31', freq='D')\n",
    "print(f\"Generadas {len(fechas)} fechas desde enero hasta diciembre 2023\")\n",
    "\n",
    "# Crear un DataFrame con datos de series temporales\n",
    "df_tiempo = pd.DataFrame({\n",
    "    'fecha': pd.date_range(start='2023-01-01', periods=365, freq='D'),\n",
    "    'ventas': np.random.normal(1000, 200, 365),\n",
    "    'temperatura': np.random.normal(20, 10, 365)\n",
    "})\n",
    "\n",
    "# Establecer fecha como índice\n",
    "df_tiempo.set_index('fecha', inplace=True)\n",
    "print(\"\\nDataFrame con índice de fecha:\")\n",
    "print(df_tiempo.head())\n",
    "\n",
    "# Extraer componentes de fecha\n",
    "df_tiempo['año'] = df_tiempo.index.year\n",
    "df_tiempo['mes'] = df_tiempo.index.month\n",
    "df_tiempo['día_semana'] = df_tiempo.index.dayofweek\n",
    "df_tiempo['nombre_mes'] = df_tiempo.index.month_name()\n",
    "\n",
    "print(\"\\nComponentes de fecha extraídos:\")\n",
    "print(df_tiempo[['año', 'mes', 'día_semana', 'nombre_mes']].head())\n",
    "\n",
    "# Resample: agregar datos por periodo\n",
    "ventas_mensuales = df_tiempo['ventas'].resample('M').agg(['sum', 'mean', 'std'])\n",
    "print(\"\\nVentas agregadas por mes:\")\n",
    "print(ventas_mensuales.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🕳️ Estrategias avanzadas para datos faltantes\n",
    "\n",
    "El manejo de datos faltantes es crucial para obtener resultados confiables en el análisis de datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un DataFrame con patrones de datos faltantes\n",
    "np.random.seed(42)\n",
    "df_faltantes = pd.DataFrame({\n",
    "    'A': np.random.randn(1000),\n",
    "    'B': np.random.randn(1000),\n",
    "    'C': np.random.randn(1000),\n",
    "    'categoria': np.random.choice(['X', 'Y', 'Z'], 1000)\n",
    "})\n",
    "\n",
    "# Introducir valores faltantes de manera realista\n",
    "missing_mask_A = np.random.random(1000) < 0.1  # 10% faltantes\n",
    "missing_mask_B = np.random.random(1000) < 0.05  # 5% faltantes\n",
    "missing_mask_C = (df_faltantes['A'] > 2) | (np.random.random(1000) < 0.03)  # Faltantes no aleatorios\n",
    "\n",
    "df_faltantes.loc[missing_mask_A, 'A'] = np.nan\n",
    "df_faltantes.loc[missing_mask_B, 'B'] = np.nan\n",
    "df_faltantes.loc[missing_mask_C, 'C'] = np.nan\n",
    "\n",
    "print(\"Patrones de datos faltantes:\")\n",
    "print(df_faltantes.isna().sum())\n",
    "print(f\"\\nTotal de filas: {len(df_faltantes)}\")\n",
    "print(f\"Filas sin valores faltantes: {len(df_faltantes.dropna())}\")\n",
    "\n",
    "# Análisis de patrones de datos faltantes\n",
    "import missingno as msno\n",
    "print(\"\\nPatrón de valores faltantes (usando missingno):\")\n",
    "# msno.matrix(df_faltantes)  # Comentado para evitar problemas de visualización\n",
    "\n",
    "# Estrategias de imputación\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "\n",
    "# 1. Imputación por media/mediana\n",
    "imputer_mean = SimpleImputer(strategy='mean')\n",
    "df_mean = df_faltantes.copy()\n",
    "df_mean[['A', 'B', 'C']] = imputer_mean.fit_transform(df_faltantes[['A', 'B', 'C']])\n",
    "\n",
    "# 2. Imputación por KNN\n",
    "imputer_knn = KNNImputer(n_neighbors=5)\n",
    "df_knn = df_faltantes.copy()\n",
    "df_knn[['A', 'B', 'C']] = imputer_knn.fit_transform(df_faltantes[['A', 'B', 'C']])\n",
    "\n",
    "# 3. Imputación por grupo\n",
    "df_group = df_faltantes.copy()\n",
    "df_group['A'] = df_group.groupby('categoria')['A'].transform(lambda x: x.fillna(x.mean()))\n",
    "df_group['B'] = df_group.groupby('categoria')['B'].transform(lambda x: x.fillna(x.median()))\n",
    "\n",
    "print(\"\\nComparación de estrategias de imputación:\")\n",
    "print(f\"Original - Media A: {df_faltantes['A'].mean():.3f}\")\n",
    "print(f\"Imputación media - Media A: {df_mean['A'].mean():.3f}\")\n",
    "print(f\"Imputación KNN - Media A: {df_knn['A'].mean():.3f}\")\n",
    "print(f\"Imputación por grupo - Media A: {df_group['A'].mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🔄 Transformaciones de datos\n",
    "\n",
    "Las transformaciones de datos nos permiten preparar los datos para análisis específicos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformaciones comunes en análisis de datos\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
    "\n",
    "# Crear datos para transformaciones\n",
    "df_transform = pd.DataFrame({\n",
    "    'valor': [1, 100, 1000, 10000, 100000],\n",
    "    'score': [0.1, 0.5, 0.8, 0.9, 0.95],\n",
    "    'categoria': ['bajo', 'medio', 'alto', 'muy_alto', 'excelente'],\n",
    "    'precio': [10.5, 25.0, 45.2, 78.9, 150.0]\n",
    "})\n",
    "\n",
    "print(\"Datos originales:\")\n",
    "print(df_transform)\n",
    "\n",
    "# 1. Transformación logarítmica\n",
    "df_transform['log_valor'] = np.log1p(df_transform['valor'])  # log(1+x) para evitar log(0)\n",
    "\n",
    "# 2. Normalización (Min-Max scaling)\n",
    "scaler_minmax = MinMaxScaler()\n",
    "df_transform['precio_norm'] = scaler_minmax.fit_transform(df_transform[['precio']])\n",
    "\n",
    "# 3. Estandarización (Z-score)\n",
    "scaler_standard = StandardScaler()\n",
    "df_transform['precio_std'] = scaler_standard.fit_transform(df_transform[['precio']])\n",
    "\n",
    "# 4. Codificación de variables categóricas\n",
    "# One-hot encoding\n",
    "categorias_dummy = pd.get_dummies(df_transform['categoria'], prefix='cat')\n",
    "df_transform = pd.concat([df_transform, categorias_dummy], axis=1)\n",
    "\n",
    "# Label encoding\n",
    "label_encoder = LabelEncoder()\n",
    "df_transform['categoria_encoded'] = label_encoder.fit_transform(df_transform['categoria'])\n",
    "\n",
    "print(\"\\nDatos después de transformaciones:\")\n",
    "print(df_transform)\n",
    "\n",
    "# 5. Binning (discretización)\n",
    "df_transform['precio_bins'] = pd.cut(df_transform['precio'], \n",
    "                                   bins=3, \n",
    "                                   labels=['Bajo', 'Medio', 'Alto'])\n",
    "\n",
    "print(\"\\nBinning de precios:\")\n",
    "print(df_transform[['precio', 'precio_bins']])\n",
    "\n",
    "# 6. Transformación de ranking\n",
    "df_transform['precio_rank'] = df_transform['precio'].rank(method='dense')\n",
    "\n",
    "print(\"\\nRanking de precios:\")\n",
    "print(df_transform[['precio', 'precio_rank']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. 📊 Visualización Avanzada\n",
    "\n",
    "La visualización efectiva es clave para comunicar insights de datos. Exploraremos técnicas avanzadas con Plotly y visualizaciones interactivas.\n",
    "\n",
    "### 🎨 Visualizaciones interactivas con Plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizaciones avanzadas con Plotly\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Crear datos más complejos para visualización\n",
    "np.random.seed(42)\n",
    "n = 500\n",
    "\n",
    "df_viz = pd.DataFrame({\n",
    "    'x': np.random.normal(0, 1, n),\n",
    "    'y': np.random.normal(0, 1, n),\n",
    "    'z': np.random.normal(0, 1, n),\n",
    "    'categoria': np.random.choice(['A', 'B', 'C', 'D'], n),\n",
    "    'tamaño': np.random.uniform(10, 100, n),\n",
    "    'fecha': pd.date_range('2023-01-01', periods=n, freq='D'),\n",
    "    'valor': np.random.exponential(2, n)\n",
    "})\n",
    "\n",
    "# 1. Scatter plot interactivo con múltiples dimensiones\n",
    "fig1 = px.scatter(df_viz, \n",
    "                  x='x', \n",
    "                  y='y', \n",
    "                  color='categoria',\n",
    "                  size='tamaño',\n",
    "                  hover_data=['z', 'valor'],\n",
    "                  title='Scatter Plot Multidimensional Interactivo')\n",
    "fig1.show()\n",
    "\n",
    "# 2. Gráfico de series temporales interactivo\n",
    "df_time_agg = df_viz.groupby(['fecha', 'categoria'])['valor'].sum().reset_index()\n",
    "\n",
    "fig2 = px.line(df_time_agg, \n",
    "               x='fecha', \n",
    "               y='valor', \n",
    "               color='categoria',\n",
    "               title='Series Temporales por Categoría')\n",
    "fig2.show()\n",
    "\n",
    "# 3. Histograma con distribuciones superpuestas\n",
    "fig3 = px.histogram(df_viz, \n",
    "                    x='valor', \n",
    "                    color='categoria',\n",
    "                    marginal='box',  # Añadir boxplot en el margen\n",
    "                    title='Distribución de Valores por Categoría')\n",
    "fig3.show()\n",
    "\n",
    "# 4. Gráfico 3D\n",
    "fig4 = px.scatter_3d(df_viz, \n",
    "                     x='x', \n",
    "                     y='y', \n",
    "                     z='z',\n",
    "                     color='categoria',\n",
    "                     size='tamaño',\n",
    "                     title='Visualización 3D')\n",
    "fig4.show()\n",
    "\n",
    "print(\"Visualizaciones interactivas creadas con Plotly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. 🌐 Extracción de Datos\n",
    "\n",
    "En el mundo real, los datos provienen de múltiples fuentes. Aprenderemos a extraer datos de APIs, páginas web y bases de datos.\n",
    "\n",
    "### 🔗 Consumo de APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracción de datos de APIs\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# Ejemplo con API pública (JSONPlaceholder)\n",
    "def obtener_datos_api():\n",
    "    \"\"\"Función para obtener datos de una API pública\"\"\"\n",
    "    try:\n",
    "        # API de ejemplo que no requiere autenticación\n",
    "        url = \"https://jsonplaceholder.typicode.com/posts\"\n",
    "        response = requests.get(url)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            datos = response.json()\n",
    "            df_api = pd.DataFrame(datos)\n",
    "            return df_api\n",
    "        else:\n",
    "            print(f\"Error en la API: {response.status_code}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error al conectar con la API: {e}\")\n",
    "        return None\n",
    "\n",
    "# Intentar obtener datos de la API\n",
    "df_posts = obtener_datos_api()\n",
    "\n",
    "if df_posts is not None:\n",
    "    print(\"Datos obtenidos de la API:\")\n",
    "    print(df_posts.head())\n",
    "    print(f\"\\nTotal de posts: {len(df_posts)}\")\n",
    "    print(f\"Columnas: {df_posts.columns.tolist()}\")\n",
    "else:\n",
    "    print(\"No se pudieron obtener datos de la API\")\n",
    "\n",
    "# Ejemplo de análisis básico de los datos de la API\n",
    "if df_posts is not None:\n",
    "    # Análisis de longitud de títulos\n",
    "    df_posts['titulo_longitud'] = df_posts['title'].str.len()\n",
    "    df_posts['body_longitud'] = df_posts['body'].str.len()\n",
    "    \n",
    "    print(\"\\nEstadísticas de longitud:\")\n",
    "    print(df_posts[['titulo_longitud', 'body_longitud']].describe())\n",
    "    \n",
    "    # Usuarios más activos\n",
    "    usuarios_activos = df_posts['userId'].value_counts().head()\n",
    "    print(\"\\nUsuarios más activos:\")\n",
    "    print(usuarios_activos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🕸️ Web Scraping con BeautifulSoup\n",
    "\n",
    "El web scraping nos permite extraer datos de páginas web. Es importante hacerlo de manera ética y respetando los términos de servicio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Web Scraping con BeautifulSoup\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "\n",
    "# Función para scraping ético\n",
    "def scraping_ejemplo():\n",
    "    \"\"\"\n",
    "    Ejemplo de web scraping básico\n",
    "    Nota: Siempre revisar robots.txt y términos de servicio\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # URL de ejemplo (sitio que permite scraping)\n",
    "        url = \"https://httpbin.org/html\"\n",
    "        \n",
    "        # Hacer la petición con headers apropiados\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "        }\n",
    "        \n",
    "        response = requests.get(url, headers=headers)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            # Extraer información\n",
    "            titulo = soup.find('title')\n",
    "            encabezados = soup.find_all(['h1', 'h2', 'h3'])\n",
    "            parrafos = soup.find_all('p')\n",
    "            \n",
    "            print(\"Scraping exitoso:\")\n",
    "            print(f\"Título: {titulo.text if titulo else 'No encontrado'}\")\n",
    "            print(f\"Número de encabezados: {len(encabezados)}\")\n",
    "            print(f\"Número de párrafos: {len(parrafos)}\")\n",
    "            \n",
    "            return True\n",
    "        else:\n",
    "            print(f\"Error al acceder a la página: {response.status_code}\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error en scraping: {e}\")\n",
    "        return False\n",
    "\n",
    "# Ejecutar ejemplo de scraping\n",
    "resultado_scraping = scraping_ejemplo()\n",
    "\n",
    "# Ejemplo de creación de datos simulados para análisis\n",
    "# (En lugar de scraping real por limitaciones del entorno)\n",
    "def crear_datos_simulados_web():\n",
    "    \"\"\"Crear datos que simularían venir de web scraping\"\"\"\n",
    "    productos = ['Laptop', 'Mouse', 'Teclado', 'Monitor', 'Auriculares']\n",
    "    tiendas = ['TiendaA', 'TiendaB', 'TiendaC']\n",
    "    \n",
    "    datos_simulados = []\n",
    "    \n",
    "    for _ in range(100):\n",
    "        producto = np.random.choice(productos)\n",
    "        tienda = np.random.choice(tiendas)\n",
    "        precio = np.random.uniform(50, 2000)\n",
    "        rating = np.random.uniform(1, 5)\n",
    "        stock = np.random.randint(0, 100)\n",
    "        \n",
    "        datos_simulados.append({\n",
    "            'producto': producto,\n",
    "            'tienda': tienda,\n",
    "            'precio': round(precio, 2),\n",
    "            'rating': round(rating, 1),\n",
    "            'stock': stock,\n",
    "            'fecha_scraping': pd.Timestamp.now()\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(datos_simulados)\n",
    "\n",
    "# Crear datos simulados\n",
    "df_scraping = crear_datos_simulados_web()\n",
    "print(\"\\nDatos simulados de web scraping:\")\n",
    "print(df_scraping.head())\n",
    "print(f\"\\nTotal de productos scrapeados: {len(df_scraping)}\")\n",
    "\n",
    "# Análisis de los datos \"scrapeados\"\n",
    "print(\"\\nAnálisis de precios por tienda:\")\n",
    "precio_por_tienda = df_scraping.groupby('tienda')['precio'].agg(['mean', 'min', 'max'])\n",
    "print(precio_por_tienda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. 🦠 Proyecto Integrador: Análisis de Datos de COVID-19\n",
    "\n",
    "Aplicaremos todas las técnicas aprendidas en un proyecto real: análisis de datos de COVID-19.\n",
    "\n",
    "### 📋 Objetivos del proyecto:\n",
    "1. Obtener datos de COVID-19 de fuentes confiables\n",
    "2. Limpiar y preparar los datos\n",
    "3. Realizar análisis exploratorio\n",
    "4. Crear visualizaciones informativas\n",
    "5. Identificar patrones y tendencias\n",
    "6. Generar insights actionables\n",
    "\n",
    "### 📊 Carga y preparación de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proyecto COVID-19: Análisis de datos completo\n",
    "\n",
    "# Como no podemos acceder a APIs externas en este entorno,\n",
    "# simularemos datos realistas de COVID-19\n",
    "def crear_datos_covid_simulados():\n",
    "    \"\"\"Crear datos realistas que simulan datos de COVID-19\"\"\"\n",
    "    \n",
    "    # Países para el análisis\n",
    "    paises = ['España', 'Francia', 'Italia', 'Alemania', 'Reino Unido', \n",
    "              'Estados Unidos', 'Brasil', 'India', 'China', 'Japón']\n",
    "    \n",
    "    # Generar fechas para un año completo\n",
    "    fechas = pd.date_range(start='2023-01-01', end='2023-12-31', freq='D')\n",
    "    \n",
    "    datos_covid = []\n",
    "    \n",
    "    for pais in paises:\n",
    "        # Simular tendencias diferentes por país\n",
    "        base_casos = np.random.randint(1000, 10000)\n",
    "        tendencia = np.random.uniform(-0.001, 0.001)\n",
    "        \n",
    "        for i, fecha in enumerate(fechas):\n",
    "            # Simular variación estacional\n",
    "            variacion_estacional = np.sin(2 * np.pi * i / 365) * 0.3\n",
    "            \n",
    "            # Casos con tendencia y ruido\n",
    "            casos = int(base_casos * (1 + tendencia * i + variacion_estacional) * \n",
    "                       (1 + np.random.normal(0, 0.2)))\n",
    "            casos = max(0, casos)  # No casos negativos\n",
    "            \n",
    "            # Muertes (aproximadamente 1-3% de casos)\n",
    "            muertes = int(casos * np.random.uniform(0.01, 0.03))\n",
    "            \n",
    "            # Recuperados (aproximadamente 95% de casos con retraso)\n",
    "            recuperados = int(casos * 0.95 * (1 + np.random.normal(0, 0.1)))\n",
    "            \n",
    "            # Población aproximada por país (en millones)\n",
    "            poblaciones = {\n",
    "                'España': 47, 'Francia': 68, 'Italia': 60, 'Alemania': 83,\n",
    "                'Reino Unido': 67, 'Estados Unidos': 331, 'Brasil': 215,\n",
    "                'India': 1380, 'China': 1441, 'Japón': 125\n",
    "            }\n",
    "            \n",
    "            datos_covid.append({\n",
    "                'fecha': fecha,\n",
    "                'pais': pais,\n",
    "                'casos_nuevos': casos,\n",
    "                'muertes_nuevas': muertes,\n",
    "                'recuperados_nuevos': recuperados,\n",
    "                'poblacion_millones': poblaciones[pais]\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(datos_covid)\n",
    "\n",
    "# Crear el dataset de COVID-19\n",
    "df_covid = crear_datos_covid_simulados()\n",
    "\n",
    "print(\"Dataset de COVID-19 creado:\")\n",
    "print(f\"Período: {df_covid['fecha'].min()} a {df_covid['fecha'].max()}\")\n",
    "print(f\"Países: {len(df_covid['pais'].unique())}\")\n",
    "print(f\"Registros totales: {len(df_covid)}\")\n",
    "print(\"\\nPrimeras filas:\")\n",
    "print(df_covid.head())\n",
    "\n",
    "# Calcular métricas acumuladas\n",
    "df_covid['casos_acumulados'] = df_covid.groupby('pais')['casos_nuevos'].cumsum()\n",
    "df_covid['muertes_acumuladas'] = df_covid.groupby('pais')['muertes_nuevas'].cumsum()\n",
    "df_covid['recuperados_acumulados'] = df_covid.groupby('pais')['recuperados_nuevos'].cumsum()\n",
    "\n",
    "# Calcular tasas por millón de habitantes\n",
    "df_covid['casos_por_millon'] = (df_covid['casos_acumulados'] / df_covid['poblacion_millones'])\n",
    "df_covid['muertes_por_millon'] = (df_covid['muertes_acumuladas'] / df_covid['poblacion_millones'])\n",
    "\n",
    "print(\"\\nMétricas calculadas:\")\n",
    "print(df_covid[['pais', 'fecha', 'casos_acumulados', 'casos_por_millon']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análisis exploratorio de datos COVID-19\n",
    "\n",
    "# 1. Resumen estadístico por país\n",
    "print(\"Resumen estadístico por país (al final del período):\")\n",
    "resumen_final = df_covid.groupby('pais').tail(1)[['pais', 'casos_acumulados', \n",
    "                                                  'muertes_acumuladas', \n",
    "                                                  'casos_por_millon']].set_index('pais')\n",
    "resumen_final = resumen_final.sort_values('casos_acumulados', ascending=False)\n",
    "print(resumen_final)\n",
    "\n",
    "# 2. Países más afectados\n",
    "print(\"\\nTop 5 países por casos totales:\")\n",
    "top_casos = resumen_final.sort_values('casos_acumulados', ascending=False).head()\n",
    "print(top_casos[['casos_acumulados', 'muertes_acumuladas']])\n",
    "\n",
    "print(\"\\nTop 5 países por casos por millón de habitantes:\")\n",
    "top_per_capita = resumen_final.sort_values('casos_por_millon', ascending=False).head()\n",
    "print(top_per_capita[['casos_por_millon']])\n",
    "\n",
    "# 3. Visualizaciones del proyecto\n",
    "\n",
    "# Configurar estilo\n",
    "plt.style.use('ggplot')\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Gráfico 1: Evolución temporal de casos por país (países principales)\n",
    "paises_principales = ['Estados Unidos', 'Brasil', 'India', 'España', 'Francia']\n",
    "for pais in paises_principales:\n",
    "    data_pais = df_covid[df_covid['pais'] == pais]\n",
    "    axes[0, 0].plot(data_pais['fecha'], data_pais['casos_acumulados'], \n",
    "                    label=pais, linewidth=2)\n",
    "\n",
    "axes[0, 0].set_title('Evolución de Casos Acumulados', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Fecha')\n",
    "axes[0, 0].set_ylabel('Casos Acumulados')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Gráfico 2: Casos por millón al final del período\n",
    "axes[0, 1].bar(range(len(top_per_capita)), top_per_capita['casos_por_millon'].values)\n",
    "axes[0, 1].set_title('Casos por Millón de Habitantes', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('País')\n",
    "axes[0, 1].set_ylabel('Casos por Millón')\n",
    "axes[0, 1].set_xticks(range(len(top_per_capita)))\n",
    "axes[0, 1].set_xticklabels(top_per_capita.index, rotation=45)\n",
    "\n",
    "# Gráfico 3: Comparación casos nuevos vs muertes nuevas\n",
    "# Promedio móvil de 7 días para suavizar\n",
    "df_covid['casos_ma7'] = df_covid.groupby('pais')['casos_nuevos'].rolling(7).mean().reset_index(0, drop=True)\n",
    "df_covid['muertes_ma7'] = df_covid.groupby('pais')['muertes_nuevas'].rolling(7).mean().reset_index(0, drop=True)\n",
    "\n",
    "# Seleccionar España para el ejemplo\n",
    "data_espana = df_covid[df_covid['pais'] == 'España']\n",
    "axes[1, 0].plot(data_espana['fecha'], data_espana['casos_ma7'], \n",
    "                label='Casos (MA7)', color='blue', linewidth=2)\n",
    "ax2 = axes[1, 0].twinx()\n",
    "ax2.plot(data_espana['fecha'], data_espana['muertes_ma7'], \n",
    "         label='Muertes (MA7)', color='red', linewidth=2)\n",
    "\n",
    "axes[1, 0].set_title('España: Casos vs Muertes (Media Móvil 7 días)', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Fecha')\n",
    "axes[1, 0].set_ylabel('Casos Nuevos', color='blue')\n",
    "ax2.set_ylabel('Muertes Nuevas', color='red')\n",
    "\n",
    "# Gráfico 4: Distribución de casos por mes\n",
    "df_covid['mes'] = df_covid['fecha'].dt.month\n",
    "casos_por_mes = df_covid.groupby('mes')['casos_nuevos'].sum()\n",
    "axes[1, 1].bar(casos_por_mes.index, casos_por_mes.values)\n",
    "axes[1, 1].set_title('Distribución de Casos por Mes', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Mes')\n",
    "axes[1, 1].set_ylabel('Total de Casos Nuevos')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 4. Análisis de correlaciones\n",
    "print(\"\\n=== ANÁLISIS DE CORRELACIONES ===\")\n",
    "correlaciones = df_covid[['casos_nuevos', 'muertes_nuevas', 'recuperados_nuevos', \n",
    "                         'poblacion_millones']].corr()\n",
    "print(\"Matriz de correlación:\")\n",
    "print(correlaciones)\n",
    "\n",
    "# 5. Identificación de patrones estacionales\n",
    "print(\"\\n=== ANÁLISIS ESTACIONAL ===\")\n",
    "casos_estacionales = df_covid.groupby(['mes'])['casos_nuevos'].agg(['mean', 'std'])\n",
    "print(\"Patrones estacionales (promedio de casos por mes):\")\n",
    "print(casos_estacionales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📋 Insights y Conclusiones del Proyecto\n",
    "\n",
    "Del análisis de datos COVID-19 podemos extraer varios insights importantes:\n",
    "\n",
    "#### 🔍 Hallazgos principales:\n",
    "1. **Variabilidad entre países**: Los patrones de contagio varían significativamente\n",
    "2. **Tendencias estacionales**: Se observan patrones estacionales en algunos países\n",
    "3. **Correlación casos-muertes**: Existe una correlación positiva pero con variaciones temporales\n",
    "4. **Impacto poblacional**: Los casos por millón de habitantes ofrecen una perspectiva más equilibrada\n",
    "\n",
    "#### 💡 Recomendaciones para análisis futuros:\n",
    "- Incorporar datos de vacunación\n",
    "- Analizar impacto de medidas de política pública\n",
    "- Estudiar correlaciones con factores socioeconómicos\n",
    "- Implementar modelos predictivos\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 Ejercicios Finales del Módulo\n",
    "\n",
    "### Ejercicio 1: Análisis personalizado\n",
    "Elige un subconjunto de países y realiza un análisis comparativo detallado incluyendo:\n",
    "- Evolución temporal\n",
    "- Tasas de mortalidad\n",
    "- Patrones estacionales\n",
    "- Visualizaciones personalizadas\n",
    "\n",
    "### Ejercicio 2: Dashboard interactivo\n",
    "Crea un dashboard interactivo usando Plotly que permita:\n",
    "- Seleccionar países\n",
    "- Filtrar por fechas\n",
    "- Comparar métricas\n",
    "- Mostrar tendencias\n",
    "\n",
    "### Ejercicio 3: Análisis predictivo básico\n",
    "Implementa un modelo simple para predecir:\n",
    "- Tendencias futuras de casos\n",
    "- Patrones estacionales\n",
    "- Comparación de diferentes enfoques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 🎉 ¡Felicitaciones! Has completado el Módulo 5\n",
    "\n",
    "### 📚 Resumen de lo aprendido:\n",
    "\n",
    "1. **✅ Fundamentos del análisis de datos**\n",
    "   - Ecosistema de Python para datos\n",
    "   - Flujo de trabajo típico\n",
    "\n",
    "2. **✅ Manipulación con Pandas**\n",
    "   - Series y DataFrames\n",
    "   - Limpieza y transformación\n",
    "   - Operaciones de agrupación\n",
    "\n",
    "3. **✅ Análisis exploratorio (EDA)**\n",
    "   - Estadísticas descriptivas\n",
    "   - Visualizaciones efectivas\n",
    "   - Identificación de patrones\n",
    "\n",
    "4. **✅ Técnicas avanzadas**\n",
    "   - Manejo de fechas y datos faltantes\n",
    "   - Transformaciones de datos\n",
    "   - Imputación inteligente\n",
    "\n",
    "5. **✅ Visualización profesional**\n",
    "   - Gráficos interactivos con Plotly\n",
    "   - Dashboards informativos\n",
    "   - Comunicación visual de insights\n",
    "\n",
    "6. **✅ Extracción de datos**\n",
    "   - Consumo de APIs\n",
    "   - Web scraping ético\n",
    "   - Integración de múltiples fuentes\n",
    "\n",
    "7. **✅ Proyecto integrador**\n",
    "   - Análisis completo de COVID-19\n",
    "   - Aplicación práctica de todas las técnicas\n",
    "   - Generación de insights actionables\n",
    "\n",
    "### 🚀 Próximo módulo: Machine Learning\n",
    "\n",
    "En el **Módulo 6** exploraremos:\n",
    "- Algoritmos de aprendizaje automático\n",
    "- Scikit-learn y modelos predictivos\n",
    "- Evaluación y validación de modelos\n",
    "- Proyectos de ML en producción\n",
    "\n",
    "### 📖 Recursos adicionales recomendados:\n",
    "\n",
    "- **Libros**: \"Python for Data Analysis\" by Wes McKinney\n",
    "- **Documentación**: Pandas, Matplotlib, Plotly\n",
    "- **Práctica**: Kaggle datasets y competiciones\n",
    "- **Comunidad**: Stack Overflow, Reddit r/MachineLearning\n",
    "\n",
    "¡Estás listo para el siguiente nivel! 🎯"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
